[["index.html", "Technical Manual Licence", " Soil Sampling Design Technical Manual Rodríguez Lado, L., Angelini, M.E, Nyapwere, N., Luotto, I., Yigini, Y. 2024-01-09 Licence The Guideline Manual is made available under the Creative Commons Attribution–NonCommercial–ShareAlike 3.0 IGO licence CC BY-NC-SA 3.0 IGO. "],["introduction.html", "Chapter 1 Introduction 1.1 Basic concepts of soil sampling 1.2 Soil Sampling for Mapping 1.3 Sampling strategies in this Technical Manual 1.4 How to use this manual 1.5 Training material", " Chapter 1 Introduction A survey involves gathering data from an object of study to support statistical analysis. In this technical manual, our focus is on soil properties, which serve as the target population. Examples of these properties are soil organic carbon, pH levels, soil fractions such as clay, silt, and sand, among others. The primary objective in soil sampling is to either create maps of these variables or verify their accuracy within existing maps. This technical manual does not aim to provide an exhaustive compilation of sampling design methodologies. For this purpose, we recommend the book of Brus (2022). Instead, our focus is on demonstrating the practical application of select soil sampling techniques that can be utilised for soil mapping or assessing the precision of existing soil maps. We emphasise the context of INSII member countries, where there is a pressing need for assistance in enhancing or establishing national soil databases and maps. In many of these cases, resource limitations, accessibility challenges, time constraints, and capacity issues pose significant hurdles. Our aim is to offer pragmatic solutions within these constraints to facilitate progress in digital soil mapping (Minasny and McBratney, 2006) efforts. 1.1 Basic concepts of soil sampling Soil sampling involves observing a subset of the larger soil population. The challenge lies in accurately inferring the characteristics of the entire population based on this limited sample. Statistical methods enable us to quantify the uncertainty inherent in these inferences. For instance, if the estimated mean nutrient level in a soil sample is marginally below a crop’s requirement, there still exists a significant probability that the actual mean of the entire population is above this threshold. This distinction between calculating descriptive statistics (direct observations from the sample) and making inferences about the broader population is fundamental in soil sampling. According to Brus (2022), sampling methodologies can broadly be categorised into two approaches: design-based and model-based. This choice is related to the selection between probability and non-probability sampling, influencing the method of statistical inference. In the design-based approach, samples are selected through probability sampling. Estimates are derived from the inclusion probabilities of these samples, as determined by the sampling design. This method does not involve using a model for estimation a parameter of the population. Conversely, the model-based approach utilizes statistical models for prediction. As these models already incorporate randomness, probability sampling is not a prerequisite. The choice of the best approach depends on the survey’s objectives, the aims of soil sampling can be categorized into estimating parameters for the entire or sub population (including accuracy evaluation of a map), or mapping the study variable. Mapping the study variable typically aligns with a model-based approach, where the focus is on predicting the variable across a fine grid that represents the study area. Both design-based and model-based approaches can be used for estimating parameters of the population or subpopulations. However, as the number of subpopulations increases, the model-based approach often becomes more appealing due to its potential for greater accuracy, depending on the model’s quality. Design-based estimates offer the advantage of an objective uncertainty assessment and almost correct confidence interval coverage. Interestingly, probability samples can be used in model-based inference as well, providing flexibility for dual aims like mapping and parameter estimation. If probability sampling is not used, design-based estimation becomes infeasible, leaving model-based prediction as the sole option. In conclusion, understanding these fundamental concepts is essential for effective and accurate soil sampling, allowing for reliable conclusions about soil properties and their implications. 1.2 Soil Sampling for Mapping In soil sampling, particularly for mapping purposes, there are specific scenarios where probability sampling may not be necessary. When utilising a statistical model containing an error term modelled by a probability distribution, the need for selecting sampling units through probability sampling diminishes. This is because statistical models facilitate making quantified statistical statements about the population without strictly needing probability-based selection of units. This approach allows for the optimization of sampling units to create the most accurate maps, such as those with the smallest squared prediction error averaged over all locations in the study area. Example of a Statistical Model for Mapping Consider a simple linear regression model: \\[ Z_k = \\beta_0 + \\beta_1 x_k + \\epsilon_k, \\] where \\(Z_k\\) is the study variable for unit \\(k\\), \\(\\beta_0\\) and \\(\\beta_1\\) are regression coefficients, \\(x_k\\) is a covariate for unit \\(k\\) as a predictor, and \\(\\epsilon_k\\) is the error at unit \\(k\\), assumed to be normally distributed with mean zero and constant variance \\(\\sigma^2\\). The independence of errors is crucial, as it implies that \\(\\text{Cov}(\\epsilon_k,\\epsilon_j)=0\\) for \\(k \\neq j\\). A comparison of a simple random sample without replacement and an optimized sample for mapping using this model demonstrates different sampling patterns and implications for the accuracy of map predictions. Optimal sampling often involves selecting units with extreme values of the covariate \\(x\\), leading to strong spatial clustering. This clustering is permissible under the assumption of independent residuals in simple linear regression models. In situations where the goal is both to map the study variable and to estimate means or totals for the entire study area or subareas, combining probability sampling with model-based approaches can be advantageous. For instance, design-based and model-assisted estimation can offer more valid and objectively assessed uncertainties in mapping soil carbon stocks while also estimating total carbon stocks. These approaches do not rely on the assumptions inherent in spatial variation models, thus avoiding potential debates over the realism of these assumptions. When selecting a suitable probability sampling design for these dual objectives, consider: Sampling designs with equal inclusion probabilities. Stratified random sampling using subareas as strata, particularly when aiming to estimate means or totals for these subareas. Geographical spreading of sampling units, potentially benefiting from spatial analysis methods like kriging. 1.3 Sampling strategies in this Technical Manual This technical manual primarily focuses on two advanced soil sampling methodologies: Conditioned Latin Hypercube Sampling (cLHS) and Stratified Simple Random Sampling. cLHS is a model-based method adapted for observational studies, especially effective in multivariate data representation. It stands out for its ability to handle multiple covariates, ensuring that samples accurately reflect the multivariate distribution of these variables. The method is characterized by its unique approach to defining intervals for each covariate, aiming to balance the representation of raster cells within these intervals. This balance is achieved through a minimization criterion that considers deviations in sample size, proportions of categorical covariates, and correlations in the sample compared to the overall population. cLHS is particularly recommended for environmental and soil studies where complex, interdependent variables must be efficiently sampled and where non-linear mapping methods are used. On the other hand, Stratified Simple Random Sampling is a design-based probability sampling technique. This method is utilized to enhance sampling precision by dividing a heterogeneous population into more homogeneous subgroups or strata, such as different soil types or land use categories. Within each stratum, simple random sampling is conducted. This approach is beneficial for increasing statistical efficiency and reducing variance within each stratum. It’s broadly applicable in various fields, notably in ecological and environmental studies, where it’s used to sample distinct sub-areas with unique characteristics. We present implementations of these methods at a national scale for real case scenarios considering different constraints. 1.4 How to use this manual The manual is structured in two parts. 'Part One' presents a methodology to evaluate the capacity of existing soil legacy data to represent the potential soil diversity within a certain study area and determine whether it is a valid set for Digital Soil Mapping purposes. We use the Kullback-Leibler divergence (KL) measurement to quantify the difference between the probability distributions of covariate values in the legacy samples set and in the whole area and determine how much information is lost when the sample set is used to approximate the diversity in the existing environmental conditions in the whole area. In 'Part Two' we present several methods for creating soil sampling designs. We start with the determination of the minimum sample size required for describing most of the environmental diversity in the area to the creation of sampling designs. We present examples of various sampling methods, ranging from traditional grid-based approaches to advanced statistical sampling strategies. We include systematic, random and stratified sampling methods, evaluating their strengths and weaknesses in the context of DSM. 1.5 Training material The manual exercises are written in the statistical environment R and run in the integrated development environment (IDE) RStudio for simplicity. Some scripts include modifications of the work from @Malone, and @Brus2022, which can be found at their respective repositories. The training material for this book is located in the Sampling-Design-TM GitHub repository. To download the input files and R scripts, clone the repository or click on this link, save the ZIP file, and extract its content in a folder, preferable located close to the root of your system, such as \"C:/GIT/\". Raster data can be also downloaded from the Google Earth repository of FAO-GPS (digital-soil-mapping-gsp-fao). Script 2 in the Annexes can be used in the code editor at Google Earth Engine to download the necessary environmental covariates. We have used a common structure for file paths in the exercises. By default, the RStudio console points to the folder where the active file is located (defined by setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) in the code). With this structure, R scripts appear in the root of the working directory and data files are in a 'data/' directory within the root, with .shp and .tif files located within the sub-folders 'data/shapes' and 'data/rasters' respectively. Following this recommendation simplifies the definition of paths and execution of the scripts. If users desire to change their storage paths, they have to properly adjust data paths in the R scripts. References "],["setting-up-the-software-environment.html", "Chapter 2 Setting-up the software environment 2.1 Use of R, RStudio and R Packages 2.2 R packages 2.3 GEE - google earth engine", " Chapter 2 Setting-up the software environment This chapter provides an overview on the software required to set-up a soil sampling design. The tools are open source and can be downloaded and installed by users following the steps that are described here. 2.1 Use of R, RStudio and R Packages R is a language and environment for statistical computing created in 1992. It provides a wide variety of statistical (e.g. linear modelling, statistical tests, time-series, classification, clustering, etc.) and graphical methods, and has been constantly extended by an exceptionally active user community. 2.1.1 Obtaining and installing R Installation files and instructions can be downloaded from the Comprehensive R Archive Network (CRAN). Go to the following link https://cran.r-project.org/ to download and install R. Pick an installation file for your operational system. Choose the “base” distribution of R (particularly if it is the first time you install R). Download the R installation file and open the file on your device. Follow the installation instructions. 2.1.2 Obtaining and installing RStudio Beginners will find it very hard to start using R because it has no Graphical User Interface (GUI). There are some GUIs which offer some of the functionality of R. RStudio makes R easier to use. It includes a code editor, debugging and visualization tools. Similar steps need to be followed to install RStudio. Go to https://www.rstudio.com/products/rstudio/download/ to download and install RStudio’s open source edition. On the download page, RStudio Desktop, Open Source License option should be selected. Pick an installation file for your platform. Follow the installation instruction on your local device. Figure 2.1: R Studio interface. The RStudio interface is structured by four compartments (see Fig. 2.1). The code editor is located in the upper left. Scripts that contain codes are displayed here. New scripts can be opened by clicking on the left most New button in the quick access tool bar (highlighted in green). Lines of code can be executed by clicking on Run (highlighted in blue) or by pressing ctrl + enter on your keyboard. The output of scripts or lines of code that are executed is displayed in the window below the code editor: the console (bottom left). This part of the interface corresponds to the R software that were installed previously. When working in R, it is central to work with so-called objects (for instance vectors, dataframes or matrices). These objects are saved in the global environment that is displayed in the top right panel. Finally, the R software offers a broad range of powerful tools for visualisation purposes. Graphs or maps that are generated by scripts/codes, are displayed in the bottom right panel. 2.1.3 Getting started with R R manuals: http://cran.r-project.org/manuals.html Contributed documentation: http://cran.r-project.org/other-docs.html Quick-R: http://www.statmethods.net/index.html Stackoverflow R community: https://stackoverflow.com/questions/tagged/r 2.2 R packages When you download R, you get the basic R system which implements the R language. R becomes more useful with the large collection of packages that extend the basic functionality of it. R packages are developed by the R community. refer to: * tidyverse book (R for data science) * caret (broad range of statistical learning functions) * R spatial: https://rspatial.org/ (R packages for spatial data operations) The primary source for R packages is CRAN’s official website, where currently about 20,250 available packages are listed. For spatial applications, various packages are available. You can obtain information about the available packages directly on CRAN with the ‘available.packages()’ function. The function returns a matrix of details corresponding to packages currently available at one or more repositories. An easier way to browse the list of packages is using the Task Views link, which groups together packages related to a given topic. Packages come along with extensive documentation that is very helpful to understand and solve error messages. To access information on functions or packages, type “?[Package or Function name]” or “??[Package or Function name]” in the console. The information on the package and/or function can then be accessed in the bottom right panel under “Help” (see Fig. 2.1). In addition to that, the R documentation website (https://www.rdocumentation.org/) provides more extensive help and gives clear overviews on all functions comprised in a certain package. 2.3 GEE - google earth engine Google earth engine (GEE) provides a large range of remote sensing datasets for users. It allows to use the GEE code editor to run computations using the Google servers. The high computational power of these servers enables users with limited computational capacities to run complex calculations. A user account must be created to use the code editor. This step can take some time. Once the account is validated, scripts can be written in the code editor using the Javascript language. An extensive array of instructions and guides are available on the platform. Alternatively, the Python language can be used to interact with the data. The code editor interface is structured by three panels and a map viewer (see Fig. 2.2). The left panel is structured in “Scripts”, “Docs”, and “Assets”. Under “Scripts” users can organize and save the scripts they wrote for specific purposes. “Docs” provides further information on so-called “server-side” functions that can be used to manipulate the data. Finally, in “Assets” users can upload own spatial data in common formats such as shapefiles (.shp) or raster files (.tif). The middle panel contains the scripts that can be run by clicking on the “Run” button. The right panel is composed of three functionalities. The “Inspector” provides basic information on a pixel of a layer displayed in the map below. The information consists of longitude, latitude, and - if layers are loaded - values of the pixel. The “Console” is the place where certain commands expressed in the code are shown. The most common expressions shown here are print() commands or figures derived from the loaded data. Finally, the “Tasks” button shows all tasks that were formulated in the code/script and are to be submitted to the server for computation. Once a task is submitted, the user has to click on the “Run” button appearing in the “Tasks” section to submit the task to the server. In addition to that, the data catalogue can be accessed via the search bar on the top of the page. Here, key information on the available datasets, origin, resolution and related publications can be found. Figure 2.2: Google Earth Engine code editor. "],["legacy_data.html", "Chapter 3 Evaluating Soil Legacy Data Sampling for DSM 3.1 Data Preparation 3.2 Representativeness of the Legacy Soil Data", " Chapter 3 Evaluating Soil Legacy Data Sampling for DSM Modelling techniques in Digital Soil Mapping involve the use of sampling point soil data, with its associated soil properties database, and a number of environmental covariates that will be used to ascertain the relationships of soil properties and the environment to then generalize the findings to locations where no samples have been compiled. In soil sampling design, a crucial issue is to determine both the locations and the number of the samples to be compiled. In an optimal situation, soil sample database should adequately cover all the environmental diversity space in the study area with a frequency relative to the extent of the diversity in the environmental covariates. When dealing with legacy soil data, a question that arises is if the data is representative of the environmental diversity within the study area. In this Chapter we present a method to answer this question and to build an alternative how many samples can be retrieved to cover the same environmental space as the existing soil data. The method follows the main findings in (Malone, Minansy and Brungard, 2019) and developed as {R} scripts. We adapted the original scripts to make use of vector '.shp' and raster '.tif' files, as these are data formats commonly used by GIS analysts and in which both soil and environmental data is often stored. We also made some changes in order to simplify the number of R packages and to avoid the use of deprecated packages as it appears in the original code. 3.1 Data Preparation We must load the required packages and data for the analyses. We make use of the packages sp and terra to manipulate spatial data, clhs for Conditioned Latin Hypercube Sampling, entropy to compute Kullback–Leibler (KL) divergence indexes, tripack for Delaunay triangulation and manipulate for interactive plotting within RStudio. Ensure that all these packages are installed in your system before the execution of the script. We define the working directory to the directory in which the actual file is located and load the soil legacy sampling points and the environmental rasters from the data folder. To avoid the definition of each environmental covariate, we first retrieve all files with the .tif extension and then create a SpatRaster object with all of them in a row. ## Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) Here we define a number of variables that will be used during the exercises in this manual. They include the path to raster and shp files, aggregation and disaggregation factors, and buffer distances to define potential sampling areas from sampling points. These variables are later described at the appropriate section in the manual. The original covariates are cropped to match the extent of a smaller area, the Nghe An province in this exercise, to simplify the computation time. Then, covariates are transformed by Principal Component Analysis to uncorrelated Principal Component scores. This ensures the use of a lower amount of raster data avoiding multicollinearity in the original covariates. We select the Principal Component rasters that capture 99% of the variability in the dataset. ## Load soil legacy point data and environmental covariates # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Aggregate stack to simplify data rasters for calculations cov.dat &lt;- aggregate(cov.dat, fact=agg.factor, fun=&quot;mean&quot;) # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Transform raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Create a raster stack to be used as input in the clhs::clhs function cov.dat.ras &lt;- raster::stack(cov.dat) # Subset rasters cov.dat &lt;- pca$PCA[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] cov.dat.ras &lt;- cov.dat.ras[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] # convert to dataframe cov.dat.df &lt;- as.data.frame(cov.dat) # Load legacy soil data p.dat &lt;- terra::vect(file.path(paste0(shp.path,&quot;/legacy_soils.shp&quot;))) Figure 3.1 shows the PCA-transformed raster layers used in the analyses. Figure 3.1: Covariates 3.2 Representativeness of the Legacy Soil Data The next step involves determining the distributions of environmental values in the soil samples data and comparing them with the existing distributions of each environmental variable to assess the representativeness of the existing soil samples in the environmental space. The comparison of distributions is performed using the Kullback–Leibler divergence (KL) distance, a measure used to quantify the difference between two probability distributions. KL–divergence compares an ‘objective’ or reference probability distribution (in this case, the distribution of covariates in the complete covariate space – P) with a ‘model’ or approximate probability distribution (the values of covariates in the soil samples – Q). The main idea is to determine how much information is lost when Q is used to approximate P. In other words, KL–divergence measures how much the Q distribution deviates from the P distribution. KL–divergence approaches 0 as the two distributions have identical quantities of information. To create a dataset with the values of the environmental parameters at the locations of the soil samples, we cross-reference soil and environmental data. First, we calculate a ‘n–matrix’ with the values of the covariates, dividing their values into ‘n’ bins according to an equal–probability distribution. Each bin captures the environmental variability within its interval in the total distribution. In this exercise, ‘n’ equals to 25. The result is a 26×4 matrix, where the rows represent the upper and lower limit of the bin (26 thresholds are required to represent 25 bins), and the columns correspond to the number of variables used as environmental proxies. ## Variability matrix in the covariates # Define Number of bins nb &lt;- 25 #quantile matrix (of the covariate data) q.mat &lt;- matrix(NA, nrow=(nb+1), ncol= nlyr(cov.dat)) j=1 for (i in 1:nlyr(cov.dat)){ #note the index start here #get a quantile matrix together of the covariates ran1 &lt;- minmax(cov.dat[[i]])[2] - minmax(cov.dat[[i]])[1] step1 &lt;- ran1/nb q.mat[,j] &lt;- seq(minmax(cov.dat[[i]])[1], to = minmax(cov.dat[[i]])[2], by =step1) j&lt;- j+1} From this matrix, we compute the hypercube matrix of covariates in the whole covariate space. ## Hypercube of &quot;objective&quot; distribution (P) – covariates # Convert SpatRaster to dataframe for calculations cov.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) cov.dat.mx &lt;- as.matrix(cov.dat.df) for (i in 1:nrow(cov.dat.mx)) { for (j in 1:ncol(cov.dat.mx)) { dd &lt;- cov.dat.mx[[i, j]] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { cov.mat[k, j] &lt;- cov.mat[k, j] + 1 } } } } } Te, we calculate the hypercube matrix of covariates in the sample space. ## Sample data hypercube h.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) for (i in 1:nrow(p.dat_I.df)) { for (j in 1:ncol(p.dat_I.df)) { dd &lt;- p.dat_I.df[i, j] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { h.mat[k, j] &lt;- h.mat[k, j] + 1 } } } } } KL–divergence We calculate the KL–divergence to measure how much the distribution of covariates in the sample space (Q) deviates from the distribution of covariates in the complete study area space (P). ## Compare covariate distributions in P and Q with Kullback–Leibler (KL) divergence kl.index &lt;-c() for(i in 1:ncol(cov.dat.df)){ kl &lt;- KL.empirical(c(cov.mat[,i]), c(h.mat[,i])) kl.index &lt;- c(kl.index,kl) klo &lt;- mean(kl.index) } #print(kl.index) # KL divergences of each covariate #print(klo) # KL divergence in the existing soil samples The KL–divergence is always greater than or equal to zero, and reaches its minimum value (zero) only when P and Q are identical. Thus, lower values of KL–divergence indicate a better match between both the sample and the study area spaces, suggesting that the sample space provides a fair representation of the environmental conditions in the study area. In this case, the KL–divergence value is 0.175, which quantifies the amount of environmental variability in the study area captured by the legacy samples. Percent of representativeness in relation to the overall environmental conditions Finally, we can assess the extent to which our legacy soil dataset represents the existing environmental conditions in the study area. We calculate the proportion of pixels in the study area that would fall within the convex hull polygon delineated based on the environmental conditions found only at the locations of the soil legacy data. The convex hull polygon is created using a Principal Component transformation of the data in the soil legacy dataset, utilizing the outer limits of the scores of the points projected onto the two main components (Fig. 3.2). Figure 3.2: PCA plot of the covariate This indicates that 95.3% of the existing conditions in the study area are encompassed within the convex hull delineated using the data from the soil samples. This percentage shows the level of adequacy of the legacy data for DSM in the area. References "],["determining-the-minimum-sampling-size.html", "Chapter 4 Determining the minimum sampling size", " Chapter 4 Determining the minimum sampling size Several strategies exist for designing soil sampling, including regular, random, and stratified sampling. Each strategy comes with its own set of advantages and limitations, which must be carefully considered before commencing a soil sampling campaign. Regular sampling, also called grid sampling, is straightforward and ensures uniform coverage, making it suitable for spatial analysis and detecting trends. However, it may introduce bias and miss small–scale variability. Generally, random sampling may require a larger number of samples to accurately capture soil variability compared to stratified sampling, which is more targeted. Nonetheless, from a statistical standpoint, random sampling is often preferred. It effectively minimizes selection bias by giving every part of the study area an equal chance of being selected. This approach yields a sample that is truly representative of the entire population, leading to more accurate, broadly applicable conclusions. Random sampling also supports valid statistical inferences, ensures reliability of results, and simplifies the estimation of errors, thereby facilitating a broad spectrum of statistical analyses. The determination of both the number and locations of soil samples is an important element in the success of any sampling campaign. The chosen strategy directly influences the representativeness and accuracy of the soil data collected, which in turn impacts the quality of the conclusions drawn from the study. In this manual, we make use of the data from Vietnam as stored in the Google Earth repository of FAO-GPS (digital-soil-mapping-gsp-fao) for the Nghe An province. We want to determine the minimal number of soil samples that must be collated to capture at least the 95% of variability within the environmental covariates. The procedure start with random distribution of a low number of samples in the area, determine the values of the spatial covariates, and compare them with those representing the whole diversity in the area at pixel scale. The comparisons are made using the 'Kullback–Leibler divergence (KL)' – a measure of how the probability distribution of the information in the samples is different from that of the Population, i.e. the covariate space. We also calculate the '% of representativeness' as the percent of variability in the covariate information for the complete area related to the variability of covariate information in the sample dataset. The initial section of the script is related to set–up options in the methodology. We load of R packages, define the working directory, load covariate data, and store it as SpatRaster object. Variables related to several aspects of the analyses, such as the aggregation factor of covariates (optional), the creation of a raster stack object(required in the clhs function), the initial and final number of samples in the trials, the increment step between trials, and the number of iterations within each trial, are also defined. # Path to rasters raster.path &lt;- &quot;data/rasters&quot; # Path to shapes shp.path &lt;- &quot;data/shapes&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; # Aggregation factor for up-scaling raster covariates (optional) agg.factor = 5 As in the previous section, covariates are PCA-transformed to avoid collinearity in the data and Principal Component rasters representing 99% of the information are retained for the analyses. ## Load raster covariate data # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Aggregate stack to simplify data rasters for calculations cov.dat &lt;- aggregate(cov.dat, fact=agg.factor, fun=&quot;mean&quot;) # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Simplify raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Create a raster stack to be used as input in the clhs::clhs function cov.dat.ras &lt;- raster::stack(cov.dat) # Subset rasters cov.dat &lt;- pca$PCA[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] cov.dat.ras &lt;- cov.dat.ras[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] # convert to dataframe cov.dat.df &lt;- as.data.frame(cov.dat) Fig. 4.1 shows the covariates retained for the analyses. plot(cov.dat) Figure 4.1: Plot of the covariates ## Define the number of samples to be tested in a loop (from initial to final) and the step of the sequence initial.n &lt;- 50 # Initial sampling size to test final.n &lt;- 250 # Final sampling size to test by.n &lt;- 10 # Increment size iters &lt;- 10 # Number of trials on each size The second section is where the analyses of divergence and representativeness of the sampling scheme are calculated. The analyses are performed in a loop using growing numbers of samples at each trial. Some empty vectors are defined to store the output results at each loop. At each trial of sample size 'N', soil samples are located at locations where the amount of information in the covariates is maximized according to the conditioned Latin Hypercube sampling method in the 'clhs' package (Roudier et al., 2011). A number of 10 replicates are calculated to determine the amount inter–variability in KL divergence and representativeness in the trial. The final results for each sample size correspond to the mean results obtained from each iteration at the corresponding sample size. The minimum sample size selected correspond to the size that accounts for at least 95% of the variability of information in the covariates within the area. The optimal sampling schema proposed correspond to the random scheme at the minimum sample size with higher value of representativeness. # Define empty vectors to store results number_of_samples &lt;- c() prop_explained &lt;- c() klo_samples &lt;-c() samples_storage &lt;- list() for (trial in seq(initial.n, final.n, by = by.n)) { for (iteration in 1:iters) { # Generate stratified clhs samples p.dat_I &lt;- clhs(cov.dat.ras, size = trial, iter = 10000, progress = FALSE, simple = FALSE) # Get covariate values for each point p.dat_I &lt;- p.dat_I$sampled_data # Get the covariate values at points as dataframe and delete NAs p.dat_I.df &lt;- as.data.frame(p.dat_I@data) %&gt;% na.omit() # Store samples as list with point coordinates samples_storage[[paste0(&quot;N&quot;, trial, &quot;_&quot;, iteration)]] &lt;- p.dat_I ## Comparison of population and sample distributions - Kullback-Leibler (KL) divergence # Define quantiles of the study area (number of bins) nb &lt;- 25 # Quantile matrix of the covariate data q.mat &lt;- matrix(NA, nrow = (nb + 1), ncol = nlyr(cov.dat)) j = 1 for (i in 1:nlyr(cov.dat)) { ran1 &lt;- minmax(cov.dat[[i]])[2] - minmax(cov.dat[[i]])[1] step1 &lt;- ran1 / nb q.mat[, j] &lt;- seq(minmax(cov.dat[[i]])[1], to = minmax(cov.dat[[i]])[2], by = step1) j &lt;- j + 1 } # q.mat # Hypercube of covariates in study area # Initialize the covariate matrix cov.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) cov.dat.mx &lt;- as.matrix(cov.dat.df) for (i in 1:nrow(cov.dat.mx)) { for (j in 1:ncol(cov.dat.mx)) { dd &lt;- cov.dat.mx[[i, j]] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { cov.mat[k, j] &lt;- cov.mat[k, j] + 1 } } } } } # Compare whole study area covariate space with the selected sample # Sample data hypercube (the same as for the raster data but on the sample data) h.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) for (i in 1:nrow(p.dat_I.df)) { for (j in 1:ncol(p.dat_I.df)) { dd &lt;- p.dat_I.df[i, j] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { h.mat[k, j] &lt;- h.mat[k, j] + 1 } } } } } ## Compute Kullback-Leibler (KL) divergence kl.index &lt;- c() for (i in 1:ncol(cov.dat.df)) { kl &lt;- KL.empirical(c(cov.mat[, i]), c(h.mat[, i])) kl.index &lt;- c(kl.index, kl) klo &lt;- mean(kl.index) } ## Calculate the proportion of &quot;env. variables&quot; in the covariate spectra that fall within the convex hull of variables in the &quot;environmental sample space&quot; # Principal component of the data sample pca.s = prcomp(p.dat_I.df, scale = TRUE, center = TRUE) scores_pca1 = as.data.frame(pca.s$x) # Plot the first 2 principal components and convex hull rand.tr &lt;- tri.mesh(scores_pca1[, 1], scores_pca1[, 2], &quot;remove&quot;) # Delaunay triangulation rand.ch &lt;- convex.hull(rand.tr, plot.it = F) # convex hull pr_poly &lt;- cbind(x = c(rand.ch$x), y = c(rand.ch$y)) # save the convex hull vertices # PCA projection of study area population onto the principal components PCA_projection &lt;- predict(pca.s, cov.dat.df) # Project study area population onto sample PC newScores = cbind(x = PCA_projection[, 1], y = PCA_projection[, 2]) # PC scores of projected population # Check which points fall within the polygon pip &lt;- point.in.polygon(newScores[, 2], newScores[, 1], pr_poly[, 2], pr_poly[, 1], mode.checked = FALSE) newScores &lt;- data.frame(cbind(newScores, pip)) klo_samples &lt;- c(klo_samples, klo) prop_explained &lt;- c(prop_explained, sum(newScores$pip) / nrow(newScores) * 100) number_of_samples &lt;- c(number_of_samples, trial) # print( # paste( # &quot;N samples = &quot;, # trial, # &quot; out of &quot;, # final.n, # &quot;; iteration = &quot;, # iteration, # &quot;; KL = &quot;, # klo, # &quot;; Proportion = &quot;, # sum(newScores$pip) / nrow(newScores) * 100 # ) # ) } } Figure 4.2 shows the distribution of covariates in the sample space, and Figure 4.3 indicates the variability in the estimations of KL divergence and repressentativeness percent in the 10 within each sample size. # Plot the polygon and all points to be checked plot(newScores[,1:2], xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(newScores[,1:2], na.rm = T), max(newScores[,1:2], na.rm = T)), ylim=c(min(newScores[,1:2], na.rm = T), max(newScores[,1:2], na.rm = T)), col=&#39;black&#39;, main=&#39;Environmental space plots over the convex hull of soil legacy data&#39;) polygon(pr_poly,col=&#39;#99999990&#39;) # # Plot points outside convex hull points(newScores[which(newScores$pip==0),1:2], col=&#39;red&#39;, pch=12, cex =1) Figure 4.2: Distribution of covariates in the sample space ## Plot dispersion on KL and % by N par(mar = c(5, 4, 5, 5)) boxplot(Perc ~ N, data=results, col = rgb(1, 0.1, 0, alpha = 0.5),ylab = &quot;%&quot;) mtext(&quot;KL divergence&quot;,side=4,line=3) # Add new plot par(new = TRUE,mar=c(5, 4, 5, 5)) # Box plot boxplot(KL ~ N, data=results, axes = FALSE,outline = FALSE, col = rgb(0, 0.8, 1, alpha = 0.5), ylab = &quot;&quot;) axis(4, at=seq(0.02, 0.36, by=.06), label=seq(0.02, 0.36, by=.06), las=3) # Draw legend par(xpd=TRUE) legend(&quot;top&quot;, inset=c(1,-.15) ,c(&quot;% coincidence&quot;, &quot;KL divergence&quot;), horiz=T,cex=.9, box.lty=0,fill = c(rgb(1, 0.1, 0, alpha = 0.5), rgb(0, 0.8, 1, alpha = 0.5))) Figure 4.3: Boxplot of the dispersion in KL and % repressentativeness in the iteration trials for each sample size par(xpd=FALSE) We determine the minimum sample size and plot the evaluation results. The following figure shows the cumulative distribution function (cdf) of the KL divergence and the % of representativeness with growing sample sizes. Representativeness increases with the increasing sample size, while KL divergence decreases as expected. The red dot identifies the trial with the minimum sample size for the area in relation to the covariates analysed. ## Plot cdf and minimum sampling point x &lt;- xx y &lt;- normalized mydata &lt;- data.frame(x,y) opti &lt;- mydata[mydata$x==minimum_n,] plot_ly(mydata, x = ~x, y = ~normalized, mode = &quot;lines+markers&quot;, type = &quot;scatter&quot;, name = &quot;CDF (1–KL divergence)&quot;) %&gt;% add_trace(x = ~x, y = ~jj, mode = &quot;lines+markers&quot;, type = &quot;scatter&quot;, yaxis = &quot;y2&quot;, name = &quot;KL divergence&quot;) %&gt;% add_trace(x = ~opti$x, y = ~opti$y, yaxis = &quot;y&quot;, mode = &quot;markers&quot;, name = &quot;Minimum N&quot;, marker = list(size = 8, color = &#39;#d62728&#39;,line = list(color = &#39;black&#39;, width = 1))) %&gt;% layout(xaxis = list(title = &quot;N&quot;, showgrid = T, dtick = 50, tickfont = list(size = 11)), yaxis = list(title = &quot;1–KL divergence (% CDF)&quot;, showgrid = F ), yaxis2 = list(title = &quot;KL divergence&quot;, overlaying = &quot;y&quot;, side = &quot;right&quot;), legend = list(orientation = &quot;h&quot;, y = 1.2, x = 0.1, traceorder = &quot;normal&quot;), margin = list(t = 50, b = 50, r = 100, l = 80), hovermode = &#39;x&#39;) %&gt;% config(displayModeBar = FALSE) Figure 4.4: KL Divergence and Proportion of Representativeness as function of sample size According to Figure 4.4, the minimum sampling size for the area, which captures at least 95% of the environmental variability of covariates is N = 162. Finally, we can determine the optimal distribution of samples over the study area according to these specific results, taking into account the minimum sampling size and the increasing interval in the sample size. The results are shown in Figure 4.5. ## Determine the optimal iteration according to the minimum N size optimal_iteration &lt;- results[which(abs(results$N - minimum_n) == min(abs(results$N - minimum_n))),] %&gt;% mutate(IDX = 1:n()) %&gt;% filter(Perc==max(Perc)) # Plot best iteration points N_final &lt;- samples_storage[paste0(&quot;N&quot;,optimal_iteration$N,&quot;_&quot;, optimal_iteration$IDX)][[1]] plot(cov.dat[[1]]) points(N_final) Figure 4.5: Covariates and optimal distribution of samples In summary, we utilize the variability within the covariate data to ascertain the minimum number of samples required to capture a minimum of 95% of this variability. Our approach involves assessing the similarities in variability between the sample space and the population space (study area) through calculations of the Kullback–Leibler (KL) divergence and the percentage of similarity at various stages of increasing sample sizes. These results are then utilized to fit a model representing the expected distribution of representativeness as a function of sample size. This model guides us in determining the minimum sample size necessary to achieve a representation of at least 95% of the environmental diversity within the area References "],["stratified-simple-random-sampling.html", "Chapter 5 Stratified Simple Random Sampling 5.1 General Procedure 5.2 Stratified random sampling 5.3 Stratified simple random sampling for large areas 5.4 Stratified simple regular sampling 5.5 Simple Random Sampling based on a stratified raster", " Chapter 5 Stratified Simple Random Sampling Stratified simple random sampling is a technique where the study area is divided into different groups or strata based on certain environmental traits and a number of random samples are taken from within each group. One of the primary advantages of stratified sampling is its ability to capture the diversity within a population by making sure each group is represented. It can provide a more accurate reflection of the entire population compared to random sampling, especially when the groups are distinct and have unique qualities. This approach is particularly beneficial when certain subgroups within the population are specifically noteworthy. It also allows for more precise estimates with a smaller total sample size compared to simple random choice. Stratified sampling presents some disadvantages. Achieving effective categories requires a proper definition and delineation of the initial information to create the strata. The classification of the environmental information into categories and ensuring fair portrayal of each can be intricate and time–taking and mislabeling elements into an improper group can lead to skewed outcomes. 5.1 General Procedure The creation of a stratified simple random sampling design involves the identification of relevant features describing the environmental diversity in the area (soil and landcover are the environmental variables generally used to define strata), delineation of the strata, determination of the number of samples to distribute to each stratum, followed by random sampling within it. By identifying relevant classes, combining them to define strata and allocating an appropriate number of samples to each stratum, a representative sample can be obtained. Random sampling within each stratum helps to ensure that the sample is unbiased and provides a fair representation of the overall conditions in the area. The first question is about how many samples must be retrieved from each strata. The sampling scheme starts with the definition of the total number of samples to collect. In this case, the determination of the sample size is a complex and highly variable process based, among others, on the specific goals of the study, the variability of environmental proxies, the statistical requirements for accuracy and confidence, as well as additional considerations such as accessibility, costs and available resources. The optimal number of samples can be determined following the method proposed in Chapter 2 of this manual. The number of samples within each stratum is calculated using an area–weighted approach taking into account the relative area of each stratum. The sampling design in this section must also comply with the following requirements: All sampling strata must have a minimum size of 100 hectares. All sampling strata must be represented by at least 2 samples. This sampling process ensures the representativeness of the environmental combinations present across the area while maintaining an efficient and feasible field sampling campaign. 5.1.1 Strata creation We must determine the kind of information that will be used to construct the strata. In this manual, we present a simple procedure to build strata based on data from two environmental layers: soil groups and landcover classification data. The information should be provided in the form of vector shapefiles with associated information databases. The data on both sets often comprises a large number of categories, that would lead to a very large number of strata. Thus, it is desirable to make an effort of aggregating similar categories within each input data set, to reduce, as much as possible, the number of categories while still capturing the most of the valuable variability in the area. The fist step is to set–up the RStudio environment and load the required packages: We must define the number of samples to distribute in the sampling design and the soil and landcover information layers to build the strata. We also define a REPLACEMENT parameter to account for a reduction of the sampling area according to a certain area using predefined bounding–box, that can be also here defined. We proceed with the calculation of soil groups. In this example, soil information is stored in the field SRG. We have analysed the extent to which the information in this field can be synthesized to eliminate redundancy when creating the strata. 1. The results are shown in 5.1 ## Plot aggregated soil classes map = leaflet(leafletOptions(minZoom = 8)) %&gt;% addTiles() mv &lt;- mapview(soil[&quot;RSG&quot;], alpha=0, homebutton=T, layer.name = &quot;soils&quot;, map=map) mv@map Figure 5.1: Plot of the soil classes #ggplot() + geom_sf(data=soil, aes(fill = factor(RSG))) A similar procedure is performed on the landcover dataset. Figure 5.2 shows the landcover classes to build the strata. # Plot map with the land cover information map = leaflet() %&gt;% addTiles() mv &lt;- mapview(lc[&quot;landcover&quot;], alpha=0, homebutton=T, layer.name = &quot;Landcover&quot;, map=map) mv@map Figure 5.2: Plot of the landcover classes #ggplot() + geom_sf(data=lc, aes(fill = factor(landcover))) To create the soil–landcover strata we must combine both classified datasets. # Combine soil and landcover layers sf_use_s2(FALSE) ## Spherical geometry (s2) switched off soil_lc &lt;- st_intersection(st_make_valid(soil), st_make_valid(lc)) ## although coordinates are longitude/latitude, st_intersection assumes that they ## are planar soil_lc$soil_lc &lt;- paste0(soil_lc$RSG, &quot;_&quot;, soil_lc$landcover) soil_lc &lt;- soil_lc %&gt;% dplyr::select(soil_lc, geometry) Finally, to comply with the initial requirements of the sampling design, we calculate the areas of each polygon, delete all features with extent lesser than 100 has. The final strata map is shown in Figure 5.3. # Plot final map of stratum map = leaflet(options = leafletOptions(minZoom = 8.3)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;, map=map) mv@map Figure 5.3: Plot of strata 5.2 Stratified random sampling This example demonstrates how to establish a stratified random sampling approach within the previously defined strata polygons. The allocation of sample points is proportionate to the stratum areas, with the condition that each stratum must contain a minimum of 2 samples. The determination of sampling points, referred to as 'target points', is made during the initial phase of the sampling design and takes into consideration factors such as the area to be sampled, budget constraints and available personnel. Additionally, a set number of 'replacement points' must be designated to act as substitutes for ‘target points’ in cases where some of the original target points cannot be accessed or sampled. These ‘replacement points’ are systematically indexed, with each index indicating which ‘target point’ it serves as a substitute for. Results are shown in Figure 5.4. map = leaflet(options = leafletOptions(minZoom = 8.3)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(sf::st_as_sf(z), zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;, &#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Samples&quot;) mv@map Figure 5.4: Plot of strata and random target and replacement points 5.3 Stratified simple random sampling for large areas The implementation of a stratified simple random sampling, along with target and replacement points, can present operating difficulties when dealing with areas of significant size and with locations that are hard to reach. To address this issue, the sampling approach can be modified by excluding areas with limited accessibility. This modification can streamline fieldwork operations and establish a feasible sampling method while still retaining the essence of the stratified simple random sampling framework. By excluding areas with limited accessibility, the sampling design can be adjusted to ensure a more practical and effective approach to data collection. Delineation of sampling accessibility: The sampling area can be further limited based on accessibility considerations. Areas with very limited accessibility, defined as regions located more than 1 kilometre away from a main road or access path, may be excluded from sampling areas. To accomplish this, a map of main roads and paths can be used to establish a sampling buffer that includes areas within a 1–kilometre buffer around the road infrastructures. This exclusion helps to eliminate the most remote and challenging–to–access areas. An additional layer of accessibility information can be incorporated based on population distribution in the country, considering that, if population is present, there is a high change that points in the surroundings can be accessible for sampling. In this case, populated nuclei are vectorized into points and a 250–meter buffer is then generated around each point. These resulting areas can be then added to the 1–kilometre buffer around the roads, which collectively defined the final sampling area. Substitution of replacement points with replacement areas in close proximity to the target points: The sampling design presented before included designated replacement points to serve as substitutes for each target point in the case that it would be inaccessible during fieldwork. However, this approach presented challenges, particularly for large areas, as the replacement point could be located far from the target point, resulting in significant logistical efforts. This limitation posed a risk of delays in completing the sampling campaign within the allocated time frame. To address this challenge, an alternative strategy is to replace the idea of replacement points with replacement areas situated in the immediate vicinity of the target point. The replacement area for each target point is now confined within a 500–meter buffer surrounding the target and falls within the same sampling stratum. This approach concentrates sampling and replacement activities within a specific geographic area, streamlining the overall process. By reducing the need for extensive travel, this method enhances efficiency and facilitates sample collection. Figure 2 illustrates the distribution of sampling points and replacement areas for visualization. Additional area exclusion: Some areas can be identified as not suitable for sampling purposes. This is the case of certain natural protected areas, conflict regions presenting risks for field operators, etc. These areas must be identified masked at an initial stage of the design to exclude them from the sampling strata. The procedure is the same as that previously presented, with the difference that buffers and exclusion areas must be masked–out from the strata map before performing the random sampling. # Compute sampling areas WITH REPLACEMENT ----- if(REPLACEMENT){ # Load strata soil_lc &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;)) # Read sampling. points from previous step z &lt;- st_read(paste0(results.path,&quot;sampling_points.shp&quot;)) # Define buffer of 500 meters (coordinate system must be in metric base) buf.samples &lt;- st_buffer(z, dist=distance.buffer) # Intersect buffers samples_buffer = st_intersection(soil_lc, buf.samples) samples_buffer &lt;- samples_buffer[samples_buffer$type==&quot;Target&quot;,] samples_buffer &lt;- samples_buffer[samples_buffer$soil_lc==samples_buffer$group,] # Save Sampling areas #st_write(samples_buffer, paste0(&#39;../soil_sampling/JAM/replacement_areas_&#39;, samples.buffer, &#39;.shp&#39;), delete_dsn = TRUE) # Write target points only targets &lt;- z[z$type==&quot;Target&quot;,] #st_write(targets, &#39;../soil_sampling/JAM/sampling_points_TAR.shp&#39;, delete_dsn = TRUE) } 5.4 Stratified simple regular sampling The procedure for creating a stratified simple regular sampling design is identical to that presented for stratified simple random sampling, with the only distinction that the locations of the sampling points are distributed in a regular spatial grid. This transformation is achieved by changing the method from ‘random’ to ‘regular’ in the spatSample functions within the script above. map = leaflet(options = leafletOptions(minZoom = 11.4)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(sf::st_as_sf(z), zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;, &#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Samples&quot;) mv@map Figure 5.5: Plot of strata and regular sampling points 5.5 Simple Random Sampling based on a stratified raster Finally, it is also possible to create a stratified area weighted random sampling using raster strata. The procedure involves the creation of the strata as a raster file and implement a random sampling using the frequencies of the strata as a guideline for distribution of the samples proportionally to their frequencies. This method is easily implemented using the package ‘sgsR’ (Goodbody, Coops and Queinnec, 2023). strata &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;),quiet = TRUE) strata$code &lt;- as.integer(strata$code) # Create stratification raster strata &lt;- rast(st_rasterize(strata[&quot;code&quot;],st_as_stars(st_bbox(strata), nx = 250, ny = 250))) names(strata) &lt;- &quot;strata&quot; strata &lt;- crop(strata, nghe, mask=TRUE) # Create stratified simple random sampling target &lt;- sample_strat( sraster = strata, nSamp = 200 ) target$type &lt;- &quot;target&quot; Figure 5.6 show the histogram of frequencies of samples over the strata categories respectively. # Histogram of frequencies calculate_representation( sraster = strata, existing = target, drop=0, plot = TRUE ) Figure 5.6: Frequencies of strata and random target samples ## # A tibble: 14 × 6 ## strata srasterFreq sampleFreq diffFreq nSamp need ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0.04 0.03 -0.01 7 2 ## 2 1 0.09 0.09 0 19 1 ## 3 2 0.64 0.59 -0.0500 129 12 ## 4 4 0.03 0.03 0 7 0 ## 5 10 0.01 0.01 0 3 0 ## 6 13 0.01 0.01 0 3 0 ## 7 14 0.01 0.01 0 2 1 ## 8 18 0.03 0.02 -0.01 5 2 ## 9 19 0.01 0.01 0 2 1 ## 10 21 0.06 0.05 -0.0100 11 3 ## 11 26 0.01 0 -0.01 1 2 ## 12 27 0.01 0.01 0 2 1 ## 13 28 0.01 0.01 0 2 1 ## 14 30 0.01 0.01 0 3 0 # Add index by strata target &lt;- target %&gt;% st_as_sf() %&gt;% dplyr::group_by(strata) %&gt;% dplyr::mutate(sample_count = sum(n), order = seq_along(strata), ID = paste0(strata, &quot;.&quot;, order)) %&gt;% vect() The construction of replacement points is straightforward by creating a new stratified set of samples on the same strata. In this case, the sample size has been incremented 3 times to create 3 replacement points for each target point. replacement &lt;- sample_strat( sraster = strata, nSamp = 200*3 ) replacement$type &lt;- &quot;replacement&quot; # Histogram of frequencies calculate_representation( sraster = strata, existing = replacement, drop=0, plot = TRUE ) Figure 5.7: Frequencies of strata and random replacement samples ## # A tibble: 14 × 6 ## strata srasterFreq sampleFreq diffFreq nSamp need ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 0.04 0.04 0 22 3 ## 2 1 0.09 0.09 0 57 -1 ## 3 2 0.64 0.63 -0.0100 386 7 ## 4 4 0.03 0.03 0 20 -1 ## 5 10 0.01 0.01 0 8 -1 ## 6 13 0.01 0.01 0 8 -1 ## 7 14 0.01 0.01 0 5 2 ## 8 18 0.03 0.02 -0.01 15 4 ## 9 19 0.01 0.01 0 6 1 ## 10 21 0.06 0.06 0 34 3 ## 11 26 0.01 0.01 0 4 3 ## 12 27 0.01 0.01 0 6 1 ## 13 28 0.01 0.01 0 7 0 ## 14 30 0.01 0.01 0 8 -1 # Add index by strata replacement &lt;- replacement %&gt;% st_as_sf() %&gt;% dplyr::group_by(strata) %&gt;% dplyr::mutate(sample_count = sum(n), order = seq_along(strata), ID = paste0(strata, &quot;.&quot;, order)) %&gt;% vect() Figures 5.7 and 5.8 show the frequencies of replacement samples over the strata categories and the distribution of target and replacement samples respectively. # Plot samples over strata # plot(strata, main=&quot;Strata and random samples&quot;) # plot(nghe[1], col=&quot;transparent&quot;, add=TRUE) # points(target,col=&quot;red&quot;) # points(replacement,col=&quot;royalblue&quot;) # legend(&quot;topleft&quot;, legend = c(&quot;target&quot;,&quot;replacement&quot;), pch = 20, xpd=NA, bg=&quot;white&quot;, col=c(&quot;red&quot;,&quot;royalblue&quot;)) map = leaflet(options = leafletOptions(minZoom = 8.3)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(target, zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Target&quot;) + mapview(replacement, zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;), cex=3, legend = TRUE,layer.name = &quot;Replacement&quot;) mv@map Figure 5.8: Plot of raster strata and sampling points The resulting target and replacement points can finally be stored as shapefiles. ## Export to shapefile writeVector(target, paste0(results.path,&quot;target_pts_raster.shp&quot;), overwrite=TRUE) writeVector(replacement, paste0(results.path,&quot;replacement_pts_raster.shp&quot;), overwrite=TRUE) References "],["conditioned-latin-hypercube-sampling.html", "Chapter 6 Conditioned Latin Hypercube Sampling 6.1 cLHS Design 6.2 Including existing legacy data in a cLHS sampling design 6.3 Working with large raster data 6.4 Implementation of cost–constrained cLHS sampling 6.5 Replacement areas in cLHS design", " Chapter 6 Conditioned Latin Hypercube Sampling Conditioned Latin Hypercube Sampling (cLHS) is an advanced statistical method used for sampling multidimensional data developed within the context of digital Soil Mapping. It’s an extension of the basic Latin Hypercube Sampling (LHS) technique, a statistical method for generating a distribution of samples of a random variable. The main advantage of LHS over simple random sampling is its ability to ensure that the entire range of the auxiliary variables are explored. It divides the range of each variable into intervals of equal probability and samples each interval. The term ‘conditioned’ refers to the way the sampling is adapted or conditioned based on specific requirements or constraints. It often involves conditioning the sampling process on one or more additional variables or criteria. This helps in generating samples that are not just representative in terms of the range of values, but also in terms of their relationships or distributions. cLHS is particularly useful for sampling from multivariate data, where there are multiple interrelated variables as it occurs in soil surveys. The main advantage of cLHS is its efficiency in sampling and its ability to better capture the structure and relationships within the data, compared to simpler sampling methods and ensures that the samples are representative not just of the range of each variable, but also of their interrelations. Detailed information on cLHS can be found in (Minasny and McBratney, 2006). In this technical manual, we primarily utilize the R implementation of cLHS provided by the sgsR package (Goodbody et al., 2023). This implementation is based on the work of (Roudier et al., 2011), also available on CRAN as the clhs package. The latter is specifically employed for some of the analyses presented in this document. 6.1 cLHS Design As for stratified sampling, the creation target points from a conditioned Latin Hypercube Sampling design involves the identification of the relevant features describing the environmental diversity in the area. In this case, the environmental parameters are incorporated in the form of raster covariates. The determination of the number of samples in the design is also required. This step can be calculated following the information already provided in this manual. With the minimum sampling size of 162 calculated before, we can conduct conditioned Latin Hypercube Sampling design for the area in the example using the R packages 'sgsR' and 'cLHS' available at CRAN. # Path to rasters raster.path &lt;- &quot;data/rasters/&quot; # Path to shapes shp.path &lt;- &quot;data/shapes/&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; # Aggregation factor for up-scaling raster covariates (optional) agg.factor = 10 # Buffer distance for replacement areas (clhs) D &lt;- 1000 # Buffer distance to calculate replacement areas # Define the minimum sample size. By default it uses the value calculated previously #minimum_n &lt;- minimum_n We use the rasters of as covariates, which we trim by the administrative boundary of the ‘Nghe An’ province as an example to speed up the calculations in the exercise. The rasters are loaded as a raster stack, masked, and subjected to PCA transformation to reduce collinearity in the dataset. The PCA components that capture 99% of the variability in the data are retained as representatives of the environmental diversity in the area. Rasters of elevation and slope are kept separately for further analyses. A shapefile of roads is also loaded to account for the sampling cost associated with walking distances from roads. # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Store elevation and slope separately elevation &lt;- cov.dat$dtm_elevation_250m slope &lt;- cov.dat$dtm_slope_250m # Load roads roads &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/roads.shp&quot;)),quiet=TRUE) roads &lt;- st_intersection(roads, nghe) # Simplify raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Subset rasters cov.dat &lt;- pca$PCA[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] # Remove pca stack rm(pca) # Aggregate stack to simplify data rasters for calculations # cov.dat &lt;- aggregate(cov.dat, fact=10, fun=&quot;mean&quot;) # Plot of covariates plot(cov.dat) Figure 6.1: Covariates The distribution of the sampling points is obtained using the 'sample_clhs'function together with the stack of raster covariates and the minimum number of samples calculated in the previous Section. The function uses a number of iterations for the Metropolis–Hastings annealing process, with a default of 10000, to determine the optimal location of samples that account for a maximum of information on the raster covariates. # Distribute sampling points with clhs pts &lt;- sample_clhs(cov.dat, nSamp = 100, iter = 10000, progress = FALSE, simple = FALSE) The distribution of points is shown in Figure 6.2. # Plot cLHS samples on map plot(cov.dat[[1]], main=&quot;cLHS samples&quot;) points(pts, col=&quot;red&quot;, pch = 1) Figure 6.2: Distribution of cLHS sampling points in the study area 6.2 Including existing legacy data in a cLHS sampling design In situations where there are legacy soil data samples available, it would be interesting to include them in the cLHS design to increase the diversity of covariates and avoid oversampling for some conditions. In this cases, the ancillary data can be included in the design as additional points to the 'sample_clhs' function. # Create an artificial random legacy dataset of 50 samples over the study area as an example legacy.data &lt;- sample_srs( raster = cov.dat, nSamp = 50) # Calculate clhs 100 points plus locations of legacy data res &lt;- sample_clhs(cov.dat, nSamp = 150, existing=legacy.data, iter = 10000, progress = FALSE) Figure 6.3 shows the distribution of the created cLHS samples, which also include the position of the original legacy soil data points. # Plot points plot(cov.dat[[1]], main=&quot;cLHS samples (blue circles) and legacy samples (red diamonds)&quot;) points(res, col=&quot;navy&quot;, pch = 1) points(res[res$type==&quot;existing&quot;,], col=&quot;red&quot;, pch = 5, cex=2) Figure 6.3: cLHS sampling points with legacy data 6.3 Working with large raster data The 'sample_clhs' function samples the covariates in the raster stack in order to determine the optimal location of samples that best represent the environmental conditions in the area. In the case of working with large raster sets, the process can be highly computing demanding since all pixels in the raster stack are used in the process. There are two simple methods to avoid this constraint: Aggregation of covariates: The quickest solution is to aggregate the covariates in the raster stack to a lower pixel resolution. This is directly performed using the 'aggregate' function from the 'terra'package. In case that the raster stack has discrete layers (factor data), the corresponding layers has to be aggregated separately using either the ‘min’ or ‘max’ functions to avoid corruption of the data and the results added later to the data of continuous raster layers. # Aggregation of covariates by a factor of 10. # The original grid resolution is up-scaled using the mean value of the pixels in the grid cov.dat2 &lt;- aggregate(cov.dat, fact=10, fun=&quot;mean&quot;) # Create clhs samples upon the resampled rasters resampled.clhs &lt;- sample_clhs(cov.dat2, nSamp = 150, iter = 10000, progress = FALSE) # Plot the points over the 1st raster plot(cov.dat2[[1]], main=&quot;cLHS samples&quot;) points(resampled.clhs , col=&quot;red&quot;, pch = 1) Figure 6.4: cLHS sampling points on up-scaled raster data rm(cov.dat2) Sampling covariate data: Other method that can be used is to sample the stack (extract the covariates information at point scale) on a regular grid at a lower resolution than the raster grid and use this information as input. The creation of a regular point grid on the raster stack is straightforward through the function spatSample from the 'terra' package. In this case we create a regular grid of 1000 points. # Create a regular grid of 1000 points on the covariate space regular.sample &lt;- spatSample(cov.dat, size = 1000, xy=TRUE, method=&quot;regular&quot;, na.rm=TRUE) # plot the points over the 1st raster plot(cov.dat[[1]], main=&quot;Regular resampled data&quot;) points(regular.sample, col=&quot;red&quot;, pch = 1) Figure 6.5: Low resolution points of covariate data This dataframe can be directly used to get locations that best represent the covariate space in the area. # Create clhs samples upon the regular grid regular.sample.clhs &lt;- clhs(regular.sample, size = 100, progress = FALSE, iter = 10000, simple = FALSE) # Plot points of clhs samples points &lt;- regular.sample.clhs$sampled_data # Get point coordinates of clhs sampling plot(cov.dat[[1]], main=&quot;cLHS samples (red) and covariate resampled points (blue)&quot;) points(regular.sample, col=&quot;navy&quot;, pch = 1) points(points, col=&quot;red&quot;, cex=1) Figure 6.6: cLHS sampling points on point-grid transformed raster covariate data Note that the sampling design follows the regular pattern of the regular grid extracted from the raster covariates 6.4 Implementation of cost–constrained cLHS sampling There are situation in which the accessibility to some locations is totally or partially restricted such as areas with steep slopes, remote areas, or areas with forbidden access, which highly compromises the sampling process. For these cases, the sampling design can constrain the points to particular locations by defining environmental layers that cause an increment in the cost efficiency of the sampling. This is done with the cost attribute in the main 'sample_clhs' function. The following example uses the raster layer “distance to roads” as a cost layer to avoid low accessible points located at large distance from roads while optimizing the representativeness of the remaining environmental covariates. # Load pre-calculated distance–to–roads surface dist2access &lt;- terra::rast(paste0(other.path,&quot;nghe_d2roads.tif&quot;)) # Add cost surface as raster layer cov.dat &lt;- c(cov.dat,dist2access) # Harmonize NAs cov.dat$dist2access &lt;- cov.dat$dist2access * cov.dat[[1]]/cov.dat[[1]] The sampling set is calculated using distance to roads as a cost surface. # Compute sampling points cost.clhs &lt;- sample_clhs(cov.dat, nSamp = minimum_n, iter = 10000, progress = FALSE, cost = &#39;dist2access&#39;, use.cpp = TRUE) ## Using `dist2access` as sampling constraint. Figure 6.7 shows the distribution of the cost constrained 'clhs' sampling over the 'cost' surface. The sampling procedure concentrates, as much as possible, sampling sites in locations with lower costs. # Plot samples plot(cov.dat[[&#39;dist2access&#39;]], main=&quot;cLHS samples with &#39;cost&#39; constraints&quot;) plot(cost.clhs, col=&quot;red&quot;, cex=1,add=T) Figure 6.7: cLHS sampling with cost layers Cost surfaces can be defined by other parameters than distances to roads. They can represent private property boundaries, slopes, presence of wetlands, etc. The package 'sgsR' implements functions to define both cost surfaces and distances to roads simultaneously. In this case, it is possible to define an inner buffer distance – i.e. the distance from the roads that should be avoided for sampling and an outer buffer – i.e. the maximum sampling distance) from roads to maximize the variability of the sampling point while considering these limits. The 'sample_clhs' function in this package also includes options to include existing legacy data in the process of clhs sampling. # Load legacy data legacy &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/legacy_soils.shp&quot;)),quiet=TRUE) # Add slope to the stack cov.dat$slope &lt;- slope # Calculate clhs points with legacy, cost and buffer to roads buff_inner=20; buff_outer=3000 # Convert roads to sf object and cast to multilinestring roads &lt;- st_as_sf(roads) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) # Calculate clhs samples using slope as cost surface, distance to roads as # access limitations, and including existing legacy data aa &lt;- sgsR::sample_clhs(mraster = cov.dat, nSamp = minimum_n, existing = legacy, iter = 10000, details = TRUE, cost=&quot;slope&quot;, access=roads, buff_inner=buff_inner, buff_outer=buff_outer) Legacy data is represented as blue dots while new samples from cLHS analyses are in red colour (Fig.6.7). Note that the new sampling points are located within a distance buffer of 20-3000 meters from roads. In addition, a cost surface has also been included in the analyses. ## Plot distances, roads, clhs points and legacy data plot(cov.dat$dist2access, main=&quot;New (red) and existing (blue) samples&quot;) plot(roads,add=TRUE, col=&quot;gray60&quot;) plot(aa$samples[aa$samples$type==&quot;new&quot;,], col= &quot;tomato&quot;,add=TRUE) plot(aa$samples[aa$samples$type==&quot;existing&quot;,], col= &quot;dodgerblue2&quot;,add=TRUE) Figure 6.8: cLHS sampling with legacy data, cost surface and distance buffers around roads The results can then be saved to a shapefile. # Write samples as shapefile aa$samples[c(&quot;type&quot;,&quot;dist2access&quot;)] %&gt;% st_write(paste0(results.path,&#39;const_clhs.shp&#39;), delete_dsn = TRUE) 6.5 Replacement areas in cLHS design The 'cLHS' package incorporates methods for the delineation of replacement locations that could be utilized in the case any sampling point is unreachable. In this case, the function determines the probability of similarity to each point in an area determined by a buffer distance around the points. The inputs of the 'similarity_buffer' functions must be a ‘RasterStack’ in the covariates and a ‘SpatialPointsDataFrame’ for the point data. ## Determine the similarity to points in a buffer of distance D gw &lt;- similarity_buffer(raster::stack(cov.dat) , as(cost.clhs, &quot;Spatial&quot;), buffer = D) The similarity probabilities for the first cLHS point is presented on Figure 6.9 over the elevation layer. # Plot results plot(elevation, legend=TRUE,main=paste(&quot;Similarity probability over elevation&quot;)) ## Overlay points points(cost.clhs, col = &quot;dodgerblue&quot;, pch = 3) ## Overlay probability stack for point 1 colors &lt;- c((RColorBrewer::brewer.pal(9, &quot;YlOrRd&quot;))) terra::plot(gw[[1]], add=TRUE , legend=FALSE, col=colors) ## Overlay 1st cLHS point points(cost.clhs[1,], col = &quot;red&quot;, pch = 12,cex=1) Figure 6.9: Probability of similarity in the buffer for the first cLHS point (in black) over elevation. The blue crosses represent the location of the remaining cLHS points from the analysis. The probabilities can then be reclassified using a threshold value to delineate the areas with higher similarity to each central target point. # Determine a threshold break to delineate replacement areas similarity_threshold &lt;- 0.90 # Reclassify buffer raster data according to the threshold break of probability # 1 = similarity &gt;= similarity_break; NA = similarity &lt; similarity_break # Define a vector with the break intervals and the output values (NA,1) breaks &lt;- c(0, similarity_threshold, NA, similarity_threshold, 1, 1) # Convert to a matrix breaks &lt;- matrix(breaks, ncol=3, byrow=TRUE) # Reclassify the data in the layers from probabilities to (NA,) s = stack(lapply(1:raster::nlayers(gw), function(i){raster::reclassify(gw[[i]], breaks, right=FALSE)})) The reclassified raster stack is then converted to an object of 'SpatialPolygonsDataFrame' class. # Polygonize replacement areas s = lapply(as.list(s), rasterToPolygons, dissolve=TRUE) s &lt;- bind(s,keepnames=TRUE) # Add the identifier of the corresponding target point for(i in 1: length(s)){ s@data$ID[i] &lt;- as.integer(stringr::str_replace(s@polygons[[i]]@ID,&quot;1.&quot;,&quot;&quot;)) } # Clean the data by storing target ID data only s@data &lt;- s@data[&quot;ID&quot;] The results are shown in Figure 6.10. # Plot results plot(cov.dat[[1]], main=paste(&quot;cLHS samples and replacement areas for threshold = &quot;, similarity_threshold)) plot(s,add=TRUE, col=NA, border=&quot;gray40&quot;) points(cost.clhs, col=&quot;red&quot;, pch = 3) Figure 6.10: Distribution of cLHS sampling points in the study area Replacement areas and sampling points can finally be stored as 'shapefiles'. # Export replacement areas to shapefile s &lt;- st_as_sf(s) st_write(s, file.path(paste0(results.path,&#39;replacement_areas_&#39;, D, &#39;.shp&#39;)), delete_dsn = TRUE) # Write cLHS sampling points to shapefile cost.clhs$ID &lt;- row(cost.clhs)[,1] # Add identifier out.pts &lt;- st_as_sf(cost.clhs) st_write(out.pts, paste0(results.path,&#39;target_clhs.shp&#39;), delete_dsn = TRUE) References "],["annex-i-compendium-of-r-scripts.html", "Annex I: Compendium of R scripts Script 1: Introduction to R Script 2: Download environmental covariates Script 3: Evaluate Legacy Data Script 4: Calculate Minimum and Optimal Sample Sizes Script 5: Stratified sampling on vector and raster data Script 6: Conditional Latin Hypercube Sampling", " Annex I: Compendium of R scripts This chapter compiles the complete list of R scripts involved in the process of soil sampling design. Script 1: Introduction to R # Introduction to R # 0. Playground ================================================= # learn important keyboard shortcuts # Ctrl + enter for running code # tab after writing the first three # characters of the function name # F1 to access the help # explore the use of &lt;-, $, [], ==, !=, c(), :, # data.frame(), list(), as.factor() a &lt;- 10:15 a[2] a[2:3] b &lt;- c(&quot;1&quot;, &quot;a&quot;, a ) length(b) df &lt;- data.frame(column_a = 1:8, column_b = b) df[,1] df$column_b as.numeric(df$column_b) plot(df) df[1:3,] df[,1] as.factor(b) d &lt;- list(a, b, df) d names(d) names(d) &lt;- c(&quot;numeric_vector&quot;, &quot;character_vector&quot;, &quot;dataframe&quot;) d d[[1]] d$numeric_vector a == b a != b # 1. Set working directory ====================================== # Set working directory to a specific location setwd(&quot;C:/GIT/Digital-Soil-Mapping/&quot;) # Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) # 2. Install and load packages ================================== # readxl, tidyverse, and data.table packages using the functions install.packages(&quot;tidyverse&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;data.table&quot;) library(tidyverse) library(readxl) library(data.table) # Install packages from other repository than CRAN install.packages(&quot;remotes&quot;) remotes::install_github(&quot;lemuscanovas/synoptReg&quot;) # 3. Import an spreadsheet ====================================== ## 3.1 Read the MS Excel file ----------------------------------- #Read the soil_data.xlsx file, spreadsheet 2, using read_excel read_excel(path = &quot;data/other/soil_data.xlsx&quot;, sheet = 2) ## 3.2 Read the csv file with the native function --------------- # 01-Data/horizon.csv read.csv(&quot;data/other/soil_profile_data.csv&quot;) ## 3.3 Read the csv file with the tidyverse function ------------ read_csv(&quot;data/other/soil_profile_data.csv&quot;) ## 3.4 Read the csv file with the data.table function ----------- fread(&quot;data/other/soil_profile_data.csv&quot;) ## 3.5 Assign the dataframe to an object called dat ------------- dat &lt;- read_csv(&quot;data/other/soil_profile_data.csv&quot;) # 4. Tidyverse functions ======================================== ## 4.1 Select pid, hip, top, bottom, ph_h2o, cec from dat ------- dat_1 &lt;- dat %&gt;% select(id_prof, id_hor, top, bottom, ph_h2o, cec) ## 4.2 Filter: pick observations by their values ---------------- # filter observations with cec &gt; 50 cmolc/100g dat_2 &lt;- dat_1 %&gt;% filter(cec &gt; 30) dat_2 ## 4.3 Mutate: create a new variable ---------------------------- # thickness = top - bottom dat_3 &lt;- dat_2 %&gt;% mutate(thickness = bottom - top) ## 4.4 Group_by and summarise ----------------------------------- # group by variable pid # summarise taking the mean of pH and cec dat_4 &lt;- dat_3 %&gt;% group_by(id_prof) %&gt;% summarise(mean_ph = mean(ph_h2o), mean_cec = mean(cec)) ## 4.5 Reshape the table using pivot_longer --------------------- # use dat_3 # put the names of the variables ph_h2o, # cec and thickness in the column # variable and keep the rest of the table. Save in dat_5 dat_5 &lt;- dat_3 %&gt;% pivot_longer(ph_h2o:thickness, names_to = &quot;soil_property&quot;, values_to = &quot;value&quot;) ## 4.6 Join the table sites.csv with dat_3 ---------------------- # Load soil_phys_data030.csv (in 01-Data folder) # Join its columns with dat_3 keeping all the rows of dat_3 # save the result as dat_6 phys &lt;- read_csv(&quot;data/other/soil_phys_data030_2.csv&quot;) phys &lt;- phys %&gt;% rename(id_prof = &quot;ProfID&quot;) dat_6 &lt;- dat_3 %&gt;% left_join(phys) # or dat_6 &lt;- phys %&gt;% right_join(dat_3) # 5. Data visualization with ggplot2 ============================ ## 5.1 1D plot: histograms -------------------------------------- # histograms of cec and ph_h2o ggplot(dat_3, aes(x=cec)) + geom_histogram() ## 5.2 2D plot: scatterplot ------------------------------------- # Scatterplot bottom vs. ph_h2o ggplot(dat_3, aes(x = bottom, y = ph_h2o)) + geom_point() # add a fitting line ggplot(dat_3, aes(x = bottom, y = ph_h2o)) + geom_point() + geom_smooth(method = &quot;lm&quot; ) ## 5.3 3D plot: scatterplot ------------------------------------- # Scatterplot bottom vs. ph_h2o, add clay as color # and size inside the # function aes() ggplot(dat_3, aes(x = bottom, y = ph_h2o, color = cec, size = cec)) + geom_point() # 6. Geospatial data with terra ================================= ## Load packages (install them if needed) library(terra) ## 6.1 Load a raster and a vector layer ------------------------- # Load data/other/landcover2013.tif using rast() function, then plot it # Load data/shapes/landcover.shp using vect() function and # plot it # explore the attributes of these layers r &lt;- rast(&quot;data/other/landcover2013.tif&quot;) plot(r) v &lt;- vect(&quot;data/shapes/landcover.shp&quot;) plot(v) ## 6.2 Load a raster and a vector layer ------------------------- # Check the current CRS (EPSG) of the raster and the vector. # Find a *projected* CRS in http://epsg.io for Vietnam and # copy the number # Check the Arguments of function project (?project) that need to # be defined # Save the new object as r_proj and v_proj # plot both objects r_proj &lt;- project(x = r, y = &quot;epsg:3405&quot;, method = &quot;near&quot;, res = 250) plot(r_proj) v_proj &lt;- project(x = v, y = &quot;epsg:3405&quot;) plot(v_proj, add = TRUE) ## 6.3 Cropping and masking a raster ---------------------------- # Compute the area of the polygons in v_proj # (search for a function) and # assign the values to a new column named area # select the largest polygon using [], $, == and max() func. # and save it as pol # crop the raster with pol using the crop() function and save #it as r_pol # mask the raster r_pol with the polygon pol and save it # with the same name # plot each result v_proj$area &lt;- expanse(v_proj, unit = &quot;ha&quot;) pol &lt;- v_proj[v_proj$area == max(v_proj$area)] plot(pol) r_pol &lt;- crop(r_proj, pol) plot(r_pol) plot(pol, add = TRUE) r_pol &lt;- mask(r_pol, pol) plot(r_pol) ## 6.4 Replace values in a raster by filtering their cells ------ # Explore the following link to understand how terra #manage cell values # https://rspatial.org/terra/pkg/4-algebra.html # Replace values lower than 5 in r+pol by 0 r_pol[r_pol$landcover2013 &lt; 5] &lt;- 0 plot(r_pol) ## 6.5 Rasterize a vector layer --------------------------------- # Use rasterize() function to convert v_proj to raster # Use r_proj as reference raster # Use field landcover to assign cell values, and plot the new map v_class &lt;- rasterize(x = v_proj, y = r_proj, field = &quot;landcover&quot; ) plot(v_class) v_class activeCat(v_class) &lt;- 1 ## 6.6 Extracting raster values using points -------------------- # Covert dat_6 to spatial points using vect() function # (check help of vect()) # Note that the EPSG number is 3405 # Save the points as s # Plot s and r_proj together in the same map (Argument add=TRUE) # Extract the values of the raster using extract() # function (check the help) # Remove the ID column of the extracted values # merge the extracted data with s using cbind() function # Convert s as a dataframe s &lt;- vect(dat_6, geom=c(&quot;x&quot;, &quot;y&quot;), crs = &quot;epsg:4326&quot;) #s &lt;- project(x = s, y = &quot;epsg:3405&quot;) plot(r_proj) plot(s, add=TRUE) x &lt;- extract(r_proj,s, ID=FALSE) s &lt;- cbind(s,x) d &lt;- as.data.frame(s) d #GGally::ggscatmat(d) ## 6.7 Zonal statistics using polygons and rasters -------------- # Use the extract() func. to estimate the mean value of # distance_proj at each polygon # Use the fun= argument (check the help) # Use the cbind() func. to merge v_proj and the extracted values # convert v_proj to a dataframe # Create a ggplot boxplot (geom_boxplot) with x=landcover # and y=dist2access distance &lt;- rast(&quot;data/other/nghe_d2roads.tif&quot;) plot(distance) distance_proj &lt;- project(x = distance, y = &quot;epsg:3405&quot;, method = &quot;bilinear&quot;, res = 250) plot(distance_proj) x &lt;- extract(distance_proj, v_proj, fun = mean, ID=FALSE) v_proj &lt;- cbind(v_proj, x) d &lt;- as_tibble(v_proj) d %&gt;% ggplot(aes(x =landcover, y = dist2access, fill = landcover)) + geom_boxplot() + ylab(&quot;Distance to roads&quot;) ## END Script 2: Download environmental covariates var assets = [&quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio1&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio12&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio13&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio14&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio16&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio17&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio5&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/bio6&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/ngd10&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/pet_penman_max&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/pet_penman_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/pet_penman_min&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/pet_penman_range&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/sfcWind_max&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/sfcWind_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/CHELSA/sfcWind_range&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_030405_500m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_030405_500m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_060708_500m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_060708_500m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_091011_500m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_091011_500m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_120102_500m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/fpar_120102_500m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_030405_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_030405_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_060708_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_060708_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_091011_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_091011_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_120102_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/lstd_120102_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_030405_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_030405_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_060708_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_060708_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_091011_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_091011_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_120102_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndlst_120102_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_030405_250m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_030405_250m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_060708_250m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_060708_250m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_091011_250m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_091011_250m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_120102_250m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/ndvi_120102_250m_sd&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/snow_cover&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/MODIS/swir_060708_500m_mean&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER/crops&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER/flooded_vegetation&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER/grass&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER/shrub_and_scrub&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/LANDCOVER/trees&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_curvature_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_downslopecurvature_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_dvm2_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_dvm_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_elevation_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_mrn_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_neg_openness_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_pos_openness_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_slope_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_tpi_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_twi_500m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_upslopecurvature_250m&quot;, &quot;projects/digital-soil-mapping-gsp-fao/assets/OPENLANDMAP/dtm_vbf_250m&quot;]; // Load borders /// Using UN 2020 (replace the countries to download) /// var ISO = [&#39;ITA&#39;]; /// var aoi = /// ee.FeatureCollection( /// &#39;projects/digital-soil-mapping-gsp-fao/assets/UN_BORDERS/BNDA_CTY&#39; /// ) /// .filter(ee.Filter.inList(&#39;ISO3CD&#39;, ISO)); /// var region = aoi.geometry(); /// Using a shapefile /// 1. Upload the borders of your countries as an asset /// 2. Replace &#39;your_shapefile&#39; with the path to your shapefile var shapefile = ee.FeatureCollection(&#39;projects/digital-soil-mapping-gsp-fao/assets/Nghe_An&#39;); var region = shapefile.geometry().bounds(); // Load assets as ImageCollection var assetsCollection = ee.ImageCollection(assets); // Clip each image in the collection to the region of interest var clippedCollection = assetsCollection.map(function(img){ return img.clip(region).toFloat(); }); // Function to replace masked values with zeroes for fpar bands function replaceMaskedFpar(img) { var allBands = img.bandNames(); var fparBands = allBands.filter(ee.Filter.stringStartsWith(&#39;item&#39;, &#39;fpar&#39;)); var nonFparBands = allBands.removeAll(fparBands); var fparImg = img.select(fparBands).unmask(0); var nonFparImg = img.select(nonFparBands); // If there are no fpar bands, return the original image var result = ee.Algorithms.If(fparBands.length().eq(0), img, nonFparImg.addBands(fparImg)); return ee.Image(result); } // Clip each image in the collection to the region of //interest and replace masked values for fpar bands var clippedCollection = assetsCollection.map(function(img){ var clippedImg = img.clip(region).toFloat(); return replaceMaskedFpar(clippedImg); }); // Stack the layers and maintain the //layer names in the final file var stacked = clippedCollection.toBands(); // Get the list of asset names var assetNames = ee.List(assets).map(function(asset) { return ee.String(asset).split(&#39;/&#39;).get(-1); }); // Rename the bands with asset names var renamed = stacked.rename(assetNames); print(renamed, &#39;Covariates to be exported&#39;) // Visualize the result // Set a visualization parameter // (you can adjust the colors as desired) var visParams = { bands: &#39;bio1&#39;, min: 0, max: 1, palette: [&#39;blue&#39;, &#39;green&#39;, &#39;yellow&#39;, &#39;red&#39;] }; // Add the layer to the map Map.centerObject(renamed, 6) Map.addLayer(renamed, visParams, &#39;Covariates&#39;); // Export the stacked image to Google Drive Export.image.toDrive({ image: renamed, description: &#39;covariates&#39;, folder: &#39;GEE&#39;, scale: 250, maxPixels: 1e13, region: region }); /* Create mask for croplands ----------------------------*/ // Load the Copernicus Global Land Service image collection var imageCollection = ee.Image(&quot;COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019&quot;) .select(&quot;discrete_classification&quot;) .clip(region) var crs = &#39;EPSG:4326&#39;; // WGS84 var res = 250; // Resolution in decimal degrees // Default resampling is nearest neighbor var image1 = imageCollection.resample() .reproject({ crs: crs, // Add your desired CRS here scale: res // Add your desired scale here }); // Reclassify the land cover classes var inList = [0, 20, 30, 40, 50, 60, 70, 80, 90, 100, 111, 112, 113, 114, 115, 116, 121, 122, 123, 124, 125, 126, 200]; var outList = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]; var FAO_lu = image1.remap(inList, outList) .toDouble() .clip(region); // print(FAO_lu) // Convert 0 to NA var mask = FAO_lu.neq(0); print(mask) FAO_lu = FAO_lu.updateMask(mask); print(FAO_lu, &quot;Mask&quot;) var visParams = { bands: &#39;remapped&#39;, min: 0, max: 1, palette: [&#39;green&#39;, &#39;yellow&#39;] }; // Add the layer to the map Map.addLayer(FAO_lu,visParams ,&#39;Mask&#39;); // Export the land cover image as a raster to Google Drive Export.image.toDrive({ image: FAO_lu, folder: &#39;GEE&#39;, description: &#39;mask&#39;, scale: res, // Add your desired scale here region: region, crs: crs, // Add your desired CRS here maxPixels: 1e13 // Add a maximum number of pixels //for export if needed }); Script 3: Evaluate Legacy Data # # Digital Soil Mapping # Soil Sampling Design # Evaluation of Legacy Data # # GSP-Secretariat # Contact: Luis.RodriguezLado@fao.org #________________________________________________________________ # Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================== # Script for evaluation the degree of representativeness of a soil legacy dataset # relative to the diversity of the environmental conditions described in a set # of rasterb covariates. # # 0 - Set working directory and load packages # 1 - User-defined variables # 2 - Extract environmental data from rasters at soil locations # 3 - Extract environmental data from rasters at soil locations # 4 - Compute variability matrix in covariates # 5 - Calculate hypercube of &quot;covariates&quot; distribution (P) # 6 - Calculate hypercube of &quot;sample&quot; distribution (Q) # 7 - Calculate Representativeness of the Legacy Dataset #________________________________________________________________ start_time &lt;- Sys.time() ## 0 - Set working directory and load packages ================================= #remotes::install_github(&quot;lemuscanovas/synoptReg&quot;) # Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) setwd(&quot;../&quot;) # Move wd down to main folder # List of packages packages &lt;- c(&quot;sp&quot;,&quot;terra&quot;,&quot;raster&quot;,&quot;sf&quot;,&quot;clhs&quot;, &quot;sgsR&quot;,&quot;entropy&quot;, &quot;tripack&quot;, &quot;manipulate&quot;,&quot;dplyr&quot;,&quot;synoptReg&quot;) # Load packages lapply(packages, require, character.only = TRUE) # Remove object to save memory space rm(packages) ## 1 - User-defined variables ================================================== # Path to rasters raster.path &lt;- &quot;data/rasters&quot; # Path to shapes shp.path &lt;- &quot;data/shapes&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; # Aggregation factor for up-scaling raster covariates (optional) agg.factor = 10 ## 2 - Prepare data ============================================================ ## Load raster covariate data # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Aggregate stack to simplify data rasters for calculations cov.dat &lt;- aggregate(cov.dat, fact=agg.factor, fun=&quot;mean&quot;) # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Transform raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Create a raster stack to be used as input in the clhs::clhs function cov.dat.ras &lt;- raster::stack(cov.dat) # Subset rasters cov.dat &lt;- pca$PCA[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] cov.dat.ras &lt;- cov.dat.ras[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] ## 3 - Extract environmental data from rasters at soil locations =============== # Load legacy soil data p.dat &lt;- terra::vect(file.path(paste0(shp.path,&quot;/legacy_soils.shp&quot;))) # Extract data p.dat_I &lt;- terra::extract(cov.dat, p.dat) p.dat_I &lt;- na.omit(p.dat_I) # Remove soil points outside study area p.dat_I.df &lt;- p.dat_I[,-1] str(p.dat_I.df) ## 4 - Compute variability matrix in the covariates ==================================== # Define Number of bins nb &lt;- 25 # Quantile matrix of the covariate data q.mat &lt;- matrix(NA, nrow = (nb + 1), ncol = nlyr(cov.dat)) j = 1 for (i in 1:nlyr(cov.dat)) { ran1 &lt;- minmax(cov.dat[[i]])[2] - minmax(cov.dat[[i]])[1] step1 &lt;- ran1 / nb q.mat[, j] &lt;- seq(minmax(cov.dat[[i]])[1], to = minmax(cov.dat[[i]])[2], by = step1) j &lt;- j + 1 } q.mat ## 5 - Calculate hypercube of &quot;covariates&quot; distribution (P) =================== # Convert SpatRaster to dataframe for calculations cov.dat.df &lt;- as.data.frame(cov.dat) cov.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) cov.dat.mx &lt;- as.matrix(cov.dat.df) for (i in 1:nrow(cov.dat.mx)) { for (j in 1:ncol(cov.dat.mx)) { dd &lt;- cov.dat.mx[[i, j]] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { cov.mat[k, j] &lt;- cov.mat[k, j] + 1 } } } } } cov.mat ## 6 - Calculate hypercube of &quot;sample&quot; distribution (Q) ======================== h.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) for (i in 1:nrow(p.dat_I.df)) { for (j in 1:ncol(p.dat_I.df)) { dd &lt;- p.dat_I.df[i, j] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { h.mat[k, j] &lt;- h.mat[k, j] + 1 } } } } } h.mat ## 7 - Calculate Representativeness of the Legacy Dataset ================== ## Calculate the proportion of &quot;variables&quot; in the covariate spectra that fall within the convex hull of variables in the &quot;environmental sample space&quot; # Principal component of the legacy data sample pca.s = prcomp(p.dat_I[,2:(ncol(cov.dat.df)+1)],scale=TRUE, center=TRUE) scores_pca1 = as.data.frame(pca.s$x) # Plot the first 2 principal components and convex hull rand.tr &lt;- tri.mesh(scores_pca1[,1],scores_pca1[,2],&quot;remove&quot;) # Delaunay triangulation rand.ch &lt;- convex.hull(rand.tr, plot.it=F) # convex hull pr_poly = cbind(x=c(rand.ch$x),y=c(rand.ch$y)) # save the convex hull vertices plot(scores_pca1[,1], scores_pca1[,2], xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2])),ylim=c(min(scores_pca1[,1:2]), max(scores_pca1[,1:2])), main=&#39;Convex hull of soil legacy data&#39;) lines(c(rand.ch$x,rand.ch$x[1]), c(rand.ch$y,rand.ch$y[1]),col=&quot;red&quot;,lwd=1) # draw the convex hull (domain of legacy data) # PCA projection of study area population onto the principal components PCA_projection &lt;- predict(pca.s, cov.dat.df) # Project study area population onto sample PC newScores = cbind(x=PCA_projection[,1],y=PCA_projection[,2]) # PC scores of projected population # Plot the polygon and all points to be checked plot(newScores, xlab=&quot;PCA 1&quot;, ylab=&quot;PCA 2&quot;, xlim=c(min(newScores[,1:2]), max(newScores[,1:2])), ylim=c(min(newScores[,1:2]), max(newScores[,1:2])), col=&#39;black&#39;, main=&#39;Environmental space plots over the convex hull of soil legacy data&#39;) polygon(pr_poly,col=&#39;#99999990&#39;) # Check which points fall within the polygon pip &lt;- point.in.polygon(newScores[,2], newScores[,1], pr_poly[,2],pr_poly[,1],mode.checked=FALSE) newScores &lt;- data.frame(cbind(newScores, pip)) # Plot points outside convex hull points(newScores[which(newScores$pip==0),1:2],pch=&#39;X&#39;, col=&#39;red&#39;) # Proportion of the conditions in the study area that fall within the convex hull of sample conditions sum(nrow(newScores[newScores$pip&gt;0,]))/nrow(newScores)*100 ## END Script 4: Calculate Minimum and Optimal Sample Sizes # # Digital Soil Mapping # Soil Sampling Design # Optimizing Sample Size # # GSP-Secretariat # Contact: Luis.RodriguezLado@fao.org #________________________________________________________________ #Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================== # The goal of this script is to determine the minimum sample size required to describe an area # while retaining for a 95% of coincidence in the environmental variability of covariates # in the area # # 0 - Set working directory and load necessary packages # 1 - User-defined variables # 2 - Import national data # 3 - Calculate the minimum sample size to describe the area # 4 - Plot covariate diversity as PCA scores # 5 - KL divergence and % similarity results for growing N samples # 6 - Model KL divergence # 7 - Determine the minimum sample size for 95% coincidence # 8 - Determine the optimal iteration according to the minimum N size # 9 - Plot minimum points from best iteration #________________________________________________________________ ## 0 - Set working directory and load necessary packages ======================= # Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) setwd(&quot;../&quot;) # Move wd down to main folder # List of packages packages &lt;- c(&quot;sp&quot;,&quot;terra&quot;,&quot;raster&quot;,&quot;sf&quot;,&quot;clhs&quot;, &quot;sgsR&quot;,&quot;entropy&quot;, &quot;tripack&quot;, &quot;manipulate&quot;,&quot;dplyr&quot;,&quot;plotly&quot;,&quot;synoptReg&quot;) # Load packages lapply(packages, require, character.only = TRUE) rm(packages) # Remove object to save memory space ## 1 - User-defined variables ================================================== # Path to rasters raster.path &lt;- &quot;data/rasters/&quot; # Path to shapes shp.path &lt;- &quot;data/shapes/&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; # Aggregation factor for up-scaling raster covariates (optional) agg.factor = 10 # Define parameters to determine minimum sampling size initial.n &lt;- 50 # Initial sampling size to test final.n &lt;- 250 # Final sampling size to test by.n &lt;- 10 # Increment size iters &lt;- 10 # Number of trials on each size ## 2 - Import national data ==================================================== ## Load raster covariate data # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Aggregate stack to simplify data rasters for calculations cov.dat &lt;- aggregate(cov.dat, fact=agg.factor, fun=&quot;mean&quot;) # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Store elevation and slope separately elevation &lt;- cov.dat$dtm_elevation_250m slope &lt;- cov.dat$dtm_slope_250m # Load roads roads &lt;- vect(file.path(paste0(shp.path,&quot;/roads.shp&quot;))) roads &lt;- crop(roads, nghe) # Simplify raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Subset rasters to main PC (var. explained &lt;=0.99) n_comps &lt;- first(which(pca$summaryPCA[3,]&gt;0.99)) cov.dat &lt;- pca$PCA[[1:n_comps]] # Plot covariates plot(cov.dat) # 3 - Calculate the minimum sample size to describe the area =================== # Start computations ---- # Initialize empty vectors to store results number_of_samples &lt;- c() prop_explained &lt;- c() klo_samples &lt;- c() samples_storage &lt;- list() # Convert SpatRaster to dataframe cov.dat.df &lt;- as.data.frame(cov.dat) # Start evaluation with growing sample sizes for (trial in seq(initial.n, final.n, by = by.n)) { for (iteration in 1:iters) { # Generate stratified clhs samples p.dat_I &lt;- sample_clhs(cov.dat, nSamp = trial, iter = 10000, progress = FALSE, simple = FALSE) # Get covariate values as dataframe and delete NAs, avoid geometry p.dat_I.df &lt;- as.data.frame(p.dat_I) %&gt;% dplyr::select(!(geometry)) %&gt;% na.omit() # Store samples as list samples_storage[[paste0(&quot;N&quot;, trial, &quot;_&quot;, iteration)]] &lt;- p.dat_I ## Comparison of population and sample distributions - Kullback-Leibler (KL) divergence # Define quantiles of the study area (number of bins) nb &lt;- 25 # Quantile matrix of the covariate data q.mat &lt;- matrix(NA, nrow = (nb + 1), ncol = nlyr(cov.dat)) j = 1 for (i in 1:nlyr(cov.dat)) { ran1 &lt;- minmax(cov.dat[[i]])[2] - minmax(cov.dat[[i]])[1] step1 &lt;- ran1 / nb q.mat[, j] &lt;- seq(minmax(cov.dat[[i]])[1], to = minmax(cov.dat[[i]])[2], by = step1) j &lt;- j + 1 } q.mat # Hypercube of covariates in study area # Initialize the covariate matrix cov.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) cov.dat.mx &lt;- as.matrix(cov.dat.df) for (i in 1:nrow(cov.dat.mx)) { for (j in 1:ncol(cov.dat.mx)) { dd &lt;- cov.dat.mx[[i, j]] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { cov.mat[k, j] &lt;- cov.mat[k, j] + 1 } } } } } cov.mat # Compare whole study area covariate space with the selected sample # Sample data hypercube (the same as for the raster data but on the sample data) h.mat &lt;- matrix(1, nrow = nb, ncol = ncol(q.mat)) for (i in 1:nrow(p.dat_I.df)) { for (j in 1:ncol(p.dat_I.df)) { dd &lt;- p.dat_I.df[i, j] if (!is.na(dd)) { for (k in 1:nb) { kl &lt;- q.mat[k, j] ku &lt;- q.mat[k + 1, j] if (dd &gt;= kl &amp;&amp; dd &lt;= ku) { h.mat[k, j] &lt;- h.mat[k, j] + 1 } } } } } h.mat ## Compute Kullback-Leibler (KL) divergence kl.index &lt;- c() for (i in 1:ncol(cov.dat.df)) { kl &lt;- KL.empirical(c(cov.mat[, i]), c(h.mat[, i])) kl.index &lt;- c(kl.index, kl) klo &lt;- mean(kl.index) } ## Calculate the proportion of &quot;env. variables&quot; in the covariate spectra that fall within the convex hull of variables in the &quot;environmental sample space&quot; # Principal component of the data sample pca.s = prcomp(p.dat_I.df, scale = TRUE, center = TRUE) scores_pca1 = as.data.frame(pca.s$x) # Plot the first 2 principal components and convex hull rand.tr &lt;- tri.mesh(scores_pca1[, 1], scores_pca1[, 2], &quot;remove&quot;) # Delaunay triangulation rand.ch &lt;- convex.hull(rand.tr, plot.it = F) # convex hull pr_poly &lt;- cbind(x = c(rand.ch$x), y = c(rand.ch$y)) # save the convex hull vertices # PCA projection of study area population onto the principal components PCA_projection &lt;- predict(pca.s, cov.dat.df) # Project study area population onto sample PC newScores = cbind(x = PCA_projection[, 1], y = PCA_projection[, 2]) # PC scores of projected population # Check which points fall within the polygon pip &lt;- point.in.polygon(newScores[, 2], newScores[, 1], pr_poly[, 2], pr_poly[, 1], mode.checked = FALSE) newScores &lt;- data.frame(cbind(newScores, pip)) klo_samples &lt;- c(klo_samples, klo) prop_explained &lt;- c(prop_explained, sum(newScores$pip) / nrow(newScores) * 100) number_of_samples &lt;- c(number_of_samples, trial) print( paste( &quot;N samples = &quot;, trial, &quot; out of &quot;, final.n, &quot;; iteration = &quot;, iteration, &quot;; KL = &quot;, klo, &quot;; Proportion = &quot;, sum(newScores$pip) / nrow(newScores) * 100 ) ) } } # 4 - Plot covariate diversity as PCA scores =================================== plot(newScores[,1:2], xlab = &quot;PCA 1&quot;, ylab = &quot;PCA 2&quot;, xlim = c(min(newScores[,1:2], na.rm = T), max(newScores[,1:2], na.rm = T)), ylim = c(min(newScores[,1:2], na.rm = T), max(newScores[,1:2], na.rm = T)), col = &#39;black&#39;, main = &#39;Environmental space plots on convex hull of soil samples&#39;) polygon(pr_poly, col = &#39;#99999990&#39;) # # Plot points outside convex hull points(newScores[which(newScores$pip == 0), 1:2], col = &#39;red&#39;, pch = 12, cex = 1) # 5 - KL divergence and % coincidence for growing N samples ============= # Merge data from number of samples, KL divergence and % coincidence results &lt;- data.frame(number_of_samples, klo_samples, prop_explained) names(results) &lt;- c(&quot;N&quot;, &quot;KL&quot;, &quot;Perc&quot;) # Calculate mean results by N size mean_result &lt;- results %&gt;% group_by(N) %&gt;% summarize_all(mean) mean_result ## Plot dispersion on KL and % by N par(mar = c(5, 4, 5, 5)) boxplot( Perc ~ N, data = results, col = rgb(1, 0.1, 0, alpha = 0.5), ylab = &quot;% coincidence&quot; ) mtext(&quot;KL divergence&quot;, side = 4, line = 3) # Add new plot par(new = TRUE, mar = c(5, 4, 5, 5)) # Box plot boxplot( KL ~ N, data = results, axes = FALSE, outline = FALSE, col = rgb(0, 0.8, 1, alpha = 0.5), ylab = &quot;&quot; ) axis( 4, at = seq(0.02, 0.36, by = .06), label = seq(0.02, 0.36, by = .06), las = 3 ) # Draw legend par(xpd=TRUE) legend(&quot;top&quot;, inset=c(1,-.15) ,c(&quot;% coincidence&quot;, &quot;KL divergence&quot;), horiz=T,cex=.9, box.lty=0,fill = c(rgb(1, 0.1, 0, alpha = 0.5), rgb(0, 0.8, 1, alpha = 0.5))) par(xpd=FALSE) # 6 - Model KL divergence ====================================================== # Create an exponential decay function of the KL divergence x &lt;- mean_result$N y = (mean_result$KL - min(mean_result$KL)) / (max(mean_result$KL) - min(mean_result$KL)) #KL # Parametrize Exponential decay function start &lt;- list() # Initialize an empty list for the starting values #fit function k = 2 b0 = 0.01 b1 = 0.01 fit1 &lt;- nls( y ~ k * exp(-b1 * x) + b0, start = list(k = k, b0 = b0, b1 = b1), control = list(maxiter = 500), trace = T ) summary(fit1) # Plot fit xx &lt;- seq(1, final.n, 1) plot(x, y) lines(xx, predict(fit1, list(x = xx))) # Predict with vfit function jj &lt;- predict(fit1, list(x = xx)) normalized = 1 - (jj - min(jj)) / (max(jj) - min(jj)) # 7 - Determine the minimum sample size to account for 95% of cumulative probability of the covariate diversity ==== minimum_n &lt;- length(which(normalized &lt; 0.95)) + 1 # Plot cdf and minimum sampling point x &lt;- xx y &lt;- normalized mydata &lt;- data.frame(x, y) opti &lt;- mydata[mydata$x == minimum_n, ] plot_ly( mydata, x = ~ x, y = ~ normalized, mode = &quot;lines+markers&quot;, type = &quot;scatter&quot;, name = &quot;CDF (1–KL divergence)&quot; ) %&gt;% add_trace( x = ~ x, y = ~ jj, mode = &quot;lines+markers&quot;, type = &quot;scatter&quot;, yaxis = &quot;y2&quot;, name = &quot;KL divergence&quot; ) %&gt;% add_trace( x = ~ opti$x, y = ~ opti$y, yaxis = &quot;y&quot;, mode = &quot;markers&quot;, name = &quot;Minimum N&quot;, marker = list( size = 8, color = &#39;#d62728&#39;, line = list(color = &#39;black&#39;, width = 1) ) ) %&gt;% layout( xaxis = list( title = &quot;N&quot;, showgrid = T, dtick = 50, tickfont = list(size = 11) ), yaxis = list(title = &quot;1–KL divergence (% CDF)&quot;, showgrid = F), yaxis2 = list( title = &quot;KL divergence&quot;, overlaying = &quot;y&quot;, side = &quot;right&quot; ), legend = list( orientation = &quot;h&quot;, y = 1.2, x = 0.1, traceorder = &quot;normal&quot; ), margin = list( t = 50, b = 50, r = 100, l = 80 ), hovermode = &#39;x&#39; ) %&gt;% config(displayModeBar = FALSE) # 8 - Determine the optimal iteration according to the minimum N size ========== optimal_iteration &lt;- results[which(abs(results$N - minimum_n) == min(abs(results$N - minimum_n))), ] %&gt;% mutate(IDX = 1:n()) %&gt;% filter(Perc == max(Perc)) # 9 - Plot minimum points from best iteration ================================== N_final &lt;- samples_storage[paste0(&quot;N&quot;, optimal_iteration$N, &quot;_&quot;, optimal_iteration$IDX)][[1]] plot(cov.dat[[1]]) plot(N_final,add=T,col=&quot;red&quot;) ## 10 - Calculate minimum and and optimal sample size with opendsm #install.packages(&#39;DescTools&#39;) library(&quot;DescTools&quot;) #&#39; Source scripts from https://github.com/newdale/opendsm/tree/main #&#39; #&#39; Determine minimum sample size for the clhs algorithm #&#39; This script is based on the publication: #&#39; Determining minimum sample size for the conditioned Latin hypercube sampling algorithm #&#39; DOI: https://doi.org/10.1016/j.pedsph.2022.09.001 source(&quot;scripts/clhs_min.R&quot;) #&#39; Optimal sample size based on the publication: #&#39; Divergence metrics for determining optimal training sample size in digital soil mapping #&#39; DOI: https://doi.org/10.1016/j.geoderma.2023.116553 source(&quot;scripts/opt_sample.R&quot;) #&#39; Perform Sequential Variable Inflation Factor Analysis source(&quot;scripts/seqVIF.R&quot;) #&#39; Function to calculate the KL-divergence #&#39; score between two probability distributions, P and Q. source(&quot;scripts/kldiv.R&quot;) #&#39; Function to calculate the Jensen-Shannon Distance #&#39; score between two probability distributions, P and Q. source(&quot;scripts/jsdist.R&quot;) #&#39; Function to calculate the Jensen-Shannon-divergence #&#39; score between two probability distributions, P and Q. source(&quot;scripts/jsdiv.R&quot;) # Convert raster covariates to dataframe cov.dat.df &lt;- data.frame(cov.dat[[1:n_comps]]) # Calculate minimum sample size min_N &lt;-clhs_min(cov.dat.df) min_n &lt;- as.numeric(min_N$Sample_Size[1]) # Calculate optimal sample size ising normalized KL-div, JS-div and JS distance opt_N &lt;- opt_sample(alg=&quot;clhs&quot;,s_min = min_n, s_max = 500, s_step=50, s_reps=4, covs = cov.dat.df,clhs_iter=100, cpus=NULL, conf=0.95) ## END Script 5: Stratified sampling on vector and raster data This script uses soil and landcover classes to define polygon ‘strata’ for an area weighted stratified sampling design. # # Digital Soil Mapping # Soil Sampling Design # Stratified Sampling on &#39;soil-landuse&#39; strata # # Contact: Luis.RodriguezLado@fao.org # Marcos.Angelini@fao.org #________________________________________________________________ # Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================================= # The goal of this script is to perform random and regular stratified # soil sampling designs using soil/landcover classes as &#39;strata&#39; # The distribution of samples on the strata is based on a # weighted area distribution with the requirement of at least 2 sampling points # by strata. Small polygons (&lt; 100 has) are eliminated from calculations. # Only &#39;strata&#39; classes prone to be samples are included in the analyses. # The final sample data includes &#39;target&#39; points - i.e. primary sampling points, # and &#39;replacement&#39; points - i.e. points to be used as replacements in case # that the corresponding &#39;target&#39; point cannot be sampled for any reason. # # 0 - Set working directory and load packages # 1 - User-defined variables # 2 - Import data # 3 - Delineate soil strata # 4 - Delineate land-use strata # 5 - Create sampling strata # 6 - Accommodate strata to requirements # 7 - Plot strata # 8 - Stratified random sampling # 9 - Plot points over strata # 10 - Calculate replacement areas for points # 11 - Stratified regular sampling #________________________________________________________________ ## 0 - Set working directory and load packages ================================= # Load packages as a vector objects packages &lt;- c(&quot;sf&quot;, &quot;terra&quot;, &quot;tidyverse&quot;, &quot;rmapshaper&quot;, &quot;units&quot;,&quot;plyr&quot;, &quot;mapview&quot;, &quot;leaflet&quot;) lapply(packages, require, character.only = TRUE) # Load packages rm(packages) # Remove object to save memory space # Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) ## 1 - User-defined variables ================================================== # Path to rasters raster.path &lt;- &quot;data/rasters&quot; # Path to shapes shp.path &lt;- &quot;data/shapes&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; n &lt;- 242 ## 2 - Import data ==================================================== # Load soil data soil &lt;- st_read(paste0(shp.path,&quot;/soils.shp&quot;),promote_to_multi = FALSE) # Load Landcover data lc &lt;- st_read(paste0(shp.path,&quot;/landcover.shp&quot;)) # Define a bounding area (optional). # If BOUNDING = TRUE, then a bounding–area shapefile (bb) must be specified # and the line in the code below must be uncommented BOUNDING = FALSE #bb &lt;- st_read(paste0(shp.path,&quot;/your_bounding_area.shp&quot;)) # Compute replacement areas (optional). # If REPLACEMENT = TRUE, then replacement areas are calculated around a # specified buffer distance from target points within the same stratum class REPLACEMENT = FALSE distance.buffer = 500 # Distance must be adjusted to the coordinate system #bb &lt;- st_read(paste0(shp.path,&quot;/your_bounding_area.shp&quot;)) # 3 - Delineate soil strata ===================================================== # Clip soil data by bounding area if defined if (BOUNDING) { soil &lt;- st_intersection(soil, bb) } else { soil &lt;- soil } # Check geometry soil &lt;- st_make_valid(soil) soil &lt;- st_cast(soil, &quot;MULTIPOLYGON&quot;) #Aggregate information by reference soil group (SRG) #unique(soil$RSG) # Remove NAs (water bodies are deleted) soil &lt;- soil[!is.na(soil$RSG),] #unique(soil$RSG) # Dissolve polygons by reference soil group soil &lt;- soil %&gt;% group_by(RSG) %&gt;% dplyr::summarize() # Write soil strata to disk st_write(soil, paste0(results.path,&quot;soil_classes.shp&quot;), delete_dsn = TRUE) ## Plot aggregated soil classes map = leaflet(leafletOptions(minZoom = 8)) %&gt;% addTiles() mv &lt;- mapview(soil[&quot;RSG&quot;], alpha=0, homebutton=T, layer.name = &quot;soils&quot;, map=map) mv@map #ggplot() + geom_sf(data=soil, aes(fill = factor(RSG))) # 4 - Delineate land-use strata ================================================= # Clip landcover data by bounding area if exist if (BOUNDING) { lc &lt;- st_intersection(lc, bb) } else { lc &lt;- lc } # Check geometry lc &lt;- st_make_valid(lc) # Agregate units unique(lc$landcover) lc$landcover[lc$landcover==&quot;Broadleaf forest&quot;]&lt;- &quot;Forest&quot; lc$landcover[lc$landcover==&quot;Needleleaf forest&quot;]&lt;- &quot;Forest&quot; lc$landcover[lc$landcover==&quot;Mixed forest&quot;]&lt;- &quot;Forest&quot; lc &lt;- lc[lc$landcover!=&quot;Water body&quot;,] unique(lc$landcover) # Dissolve polygons by reference soil group lc &lt;- lc %&gt;% group_by(landcover) %&gt;% dplyr::summarize() # Write landcover strata as shapefile st_write(lc, paste0(results.path,&quot;lc_classes.shp&quot;), delete_dsn = TRUE) # Plot map with the land cover information map = leaflet() %&gt;% addTiles() mv &lt;- mapview(lc[&quot;landcover&quot;], alpha=0, homebutton=T, layer.name = &quot;Landcover&quot;, map=map) mv@map #ggplot() + geom_sf(data=lc, aes(fill = factor(landcover))) # 5 - Create sampling strata =================================================== # Combine soil and landcover layers sf_use_s2(FALSE) soil_lc &lt;- st_intersection(st_make_valid(soil), st_make_valid(lc)) soil_lc$soil_lc &lt;- paste0(soil_lc$RSG, &quot;_&quot;, soil_lc$landcover) soil_lc &lt;- soil_lc %&gt;% dplyr::select(soil_lc, geometry) # 6 - Accommodate strata to requirements ======================================= # Select by Area. Convert to area to ha and select polygons with more than 100 has soil_lc$area &lt;- st_area(soil_lc)/10000 soil_lc$area &lt;- as.vector(soil_lc$area) soil_lc &lt;- soil_lc %&gt;% group_by(soil_lc) %&gt;% mutate(area = sum(area)) soil_lc &lt;- soil_lc[soil_lc$area &gt; 100,] plot(soil_lc[1]) # Replace blank spaces with underscore symbol to keep names uniform soil_lc$soil_lc &lt;- str_replace_all(soil_lc$soil_lc, &quot; &quot;, &quot;_&quot;) # Create a column of strata numeric codes soil_lc$code &lt;- as.character(as.numeric(as.factor(soil_lc$soil_lc))) # List final strata unique(soil_lc$soil_lc) # Write final sampling strata map st_write(soil_lc, paste0(results.path,&quot;strata.shp&quot;), delete_dsn = TRUE) # 7 - Plot strata ============================================================== # Plot final map of stratum map = leaflet(options = leafletOptions(minZoom = 8.3)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;, map=map) mv@map # 8 - Stratified random sampling =============================================== # Read already created strata shapefile polygons &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;)) if(REPLACEMENT){ polygons = st_intersection(polygons,distance.buffer) } # Calculate the area of each polygon polygons$area &lt;- st_area(polygons) # Create a new column to group polygons by a common attribute polygons$group &lt;- polygons$soil_lc # Drop units to allow computations polygons &lt;- drop_units(polygons) # Calculate the total area of all polygons in each group group_areas &lt;- polygons %&gt;% dplyr::group_by(group) %&gt;% dplyr::summarize(total_area = sum(area)) # Add a code to each group group_codes &lt;- polygons %&gt;% group_by(group) %&gt;% dplyr::summarize(code = first(code)) # Join polygon strata and codes group_areas &lt;- left_join(group_areas,st_drop_geometry(group_codes), by = &quot;group&quot;) # Ensure minimum of 2 samples at each polygon in each group group_areas$sample_count &lt;- 2 # Calculate the number of samples per group based on relative area group_areas$sample_count &lt;- group_areas$sample_count + round(group_areas$total_area/sum(group_areas$total_area) * (n-sum(group_areas$sample_count))) # Adjust sample size on classes while (sum(group_areas$sample_count) != n) { if (sum(group_areas$sample_count) &gt; n) { # Reduce sample count for the largest polygon until total count is n max_index &lt;- which.max(group_areas$sample_count) group_areas$sample_count[max_index] &lt;- group_areas$sample_count[max_index] - 1 } else { # Increase sample count for the smallest polygon until total count is n min_index &lt;- which.min(group_areas$sample_count) group_areas$sample_count[min_index] &lt;- group_areas$sample_count[min_index] + 1 } } # Count the total samples. It must be equal to the sampling size sum(group_areas$sample_count) polygons &lt;- left_join(polygons, st_drop_geometry(group_areas), by = c(&quot;soil_lc&quot;=&quot;group&quot;)) polygons &lt;- dplyr::select(polygons, soil_lc, code.x, sample_count, geometry) # Generate random points within each strata of size 3 times #the required samples for each strata x &lt;- spatSample(x = vect(group_areas), size = group_areas$sample_count * 3, method = &quot;random&quot;) # Compute sampling points for strata z &lt;- x %&gt;% st_as_sf() %&gt;% dplyr::group_by(code) %&gt;% dplyr::mutate(sample_count = as.numeric(sample_count), order = seq_along(code), ID = paste0(code, &quot;.&quot;, order), type = ifelse(sample_count &gt;= order, &quot;Target&quot;, &quot;Replacement&quot;)) %&gt;% vect() # Find classes with missing samples missing.data &lt;- left_join(group_areas,data.frame(z) %&gt;% dplyr::filter(type==&quot;Target&quot;) %&gt;% dplyr::group_by(code) %&gt;% tally()) %&gt;% dplyr::mutate(diff=sample_count-n) # Determine missing sampled strata missing.strata &lt;- which(is.na(missing.data$diff)) # Determine missing sampling point in strata (undersampled strata) missing.sample = which(missing.data$diff != 0) missing.number &lt;- as.numeric(unlist(st_drop_geometry(missing.data[(missing.sample &lt;- which(missing.data$diff != 0)),7]))) # Compute sampling points for missing sampled strata x.missing.strata &lt;- x[1] x.missing.strata$sample_count&lt;- 0 for(i in missing.strata){ xx.missing.strata &lt;- x[1] xx.missing.strata$sample_count&lt;- 0 nn=0 while (sum(xx.missing.strata$sample_count) &lt; group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*5) { while(nn &lt; group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3){ my.missing.strata &lt;- spatSample(x = vect(group_areas[group_areas$code %in% i,]), size = group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*5, method = &quot;random&quot;) nn &lt;- nn + nrow(data.frame(my.missing.strata)) } xx.missing.strata &lt;- rbind(xx.missing.strata,my.missing.strata) print(sum(xx.missing.strata$sample_count)) } print(i) print(xx.missing.strata) x.missing.strata &lt;- rbind(x.missing.strata,xx.missing.strata) } # Join initial sampling with missing sampling strata data x &lt;- rbind(x, x.missing.strata) # Compute sampling points for missing samples (random sampling) x.missing.sample &lt;- x[1] for(i in missing.sample){ xx.missing.sample &lt;- x[1] xx.missing.sample$sample_count&lt;- 0 while (sum(xx.missing.sample$sample_count) &lt; (group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3)) { my.missing.sample &lt;- spatSample(x = vect(group_areas[group_areas$code %in% i,]), size = as.numeric(vect(group_areas[group_areas$code %in% i,])[[4]])+ (group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3), method = &quot;random&quot;) xx.missing.sample &lt;- rbind(xx.missing.sample,my.missing.sample) print(sum(xx.missing.sample$sample_count)) } print(i) print(xx.missing.sample) x.missing.sample &lt;- rbind(x.missing.sample,xx.missing.sample) } # Join initial sampling with missing sampling strata data and with missing samples x &lt;- rbind(x, x.missing.sample) # Remove extra artificial replacements x &lt;- x[x$sample_count &gt; 0,] # Convert and export to shapefile z &lt;- x %&gt;% st_as_sf() %&gt;% dplyr::group_by(code) %&gt;% dplyr::mutate(sample_count = as.numeric(sample_count), order = seq_along(code), ID = paste0(code, &quot;.&quot;, order), type = ifelse(sample_count &gt;= order, &quot;Target&quot;, &quot;Replacement&quot;)) %&gt;% vect() writeVector(z, paste0(results.path,&quot;strat_randm_samples.shp&quot;, overwrite=TRUE)) # Check if the number of initial target points equals the final target points n==nrow(z[z$type==&quot;Target&quot;,]) n;nrow(z[z$type==&quot;Target&quot;,]) # 9 - Plot points over strata ================================================== map = leaflet(options = leafletOptions(minZoom = 11.4)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(sf::st_as_sf(z), zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;, &#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Samples&quot;) mv@map # 10 - Calculate replacement areas for points ================================== if(REPLACEMENT){ # Load strata soil_lc &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;)) # Read sampling. points from previous step z &lt;- st_read(paste0(results.path,&quot;strat_randm_samples.shp&quot;)) # Apply buffer of 500 meters buf.samples &lt;- st_buffer(z, dist=distance.buffer) # Intersect buffers samples_buffer = st_intersection(soil_lc, buf.samples) samples_buffer &lt;- samples_buffer[samples_buffer$type==&quot;Target&quot;,] samples_buffer &lt;- samples_buffer[samples_buffer$soil_lc==samples_buffer$group,] # Save Sampling areas st_write(samples_buffer, paste0(results.path,&quot;replacement_areas.shp&quot;), delete_dsn = TRUE) # Write target points only targets &lt;- z[z$type==&quot;Target&quot;,] st_write(targets, paste0(results.path,&quot;target_points.shp&quot;), delete_dsn = TRUE) } # 11 - Stratified regular sampling ================================== # Read already created strata shapefile polygons &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;)) if(REPLACEMENT){ polygons = st_intersection(polygons,distance.buffer) } # Calculate the area of each polygon polygons$area &lt;- st_area(polygons) # Create a new column to group polygons by a common attribute polygons$group &lt;- polygons$soil_lc # Drop units to allow computations polygons &lt;- drop_units(polygons) # Calculate the total area of all polygons in each group group_areas &lt;- polygons %&gt;% dplyr::group_by(group) %&gt;% dplyr::summarize(total_area = sum(area)) # Add a code to each group group_codes &lt;- polygons %&gt;% group_by(group) %&gt;% dplyr::summarize(code = first(code)) # Join polygon strata and codes group_areas &lt;- left_join(group_areas,st_drop_geometry(group_codes), by = &quot;group&quot;) # Ensure minimum of 2 samples at each polygon in each group group_areas$sample_count &lt;- 2 # Calculate the number of samples per group based on relative area group_areas$sample_count &lt;- group_areas$sample_count + round(group_areas$total_area/sum(group_areas$total_area) * (n-sum(group_areas$sample_count))) # Adjust sample size on classes while (sum(group_areas$sample_count) != n) { if (sum(group_areas$sample_count) &gt; n) { # Reduce sample count for the largest polygon until total count is n max_index &lt;- which.max(group_areas$sample_count) group_areas$sample_count[max_index] &lt;- group_areas$sample_count[max_index] - 1 } else { # Increase sample count for the smallest polygon until total count is n min_index &lt;- which.min(group_areas$sample_count) group_areas$sample_count[min_index] &lt;- group_areas$sample_count[min_index] + 1 } } # Count the total samples. It must be equal to the sampling size sum(group_areas$sample_count) polygons &lt;- left_join(polygons, st_drop_geometry(group_areas), by = c(&quot;soil_lc&quot;=&quot;group&quot;)) polygons &lt;- dplyr::select(polygons, soil_lc, code.x, sample_count, geometry) # Generate regular points within each strata of size 3 times #the required samples for each strata x &lt;- spatSample(x = vect(group_areas), size = group_areas$sample_count * 3, method = &quot;regular&quot;) # Compute sampling points for strata z &lt;- x %&gt;% st_as_sf() %&gt;% dplyr::group_by(code) %&gt;% dplyr::mutate(sample_count = as.numeric(sample_count), order = seq_along(code), ID = paste0(code, &quot;.&quot;, order), type = ifelse(sample_count &gt;= order, &quot;Target&quot;, &quot;Replacement&quot;)) %&gt;% vect() # Find classes with missing samples missing.data &lt;- left_join(group_areas,data.frame(z) %&gt;% dplyr::filter(type==&quot;Target&quot;) %&gt;% dplyr::group_by(code) %&gt;% tally()) %&gt;% dplyr::mutate(diff=sample_count-n) # Determine missing sampled strata missing.strata &lt;- which(is.na(missing.data$diff)) # Determine missing sampling point in strata (undersampled strata) missing.sample = which(missing.data$diff != 0) missing.number &lt;- as.numeric(unlist(st_drop_geometry(missing.data[(missing.sample &lt;- which(missing.data$diff != 0)),7]))) # Compute sampling points for missing sampled strata x.missing.strata &lt;- x[1] x.missing.strata$sample_count&lt;- 0 for(i in missing.strata){ xx.missing.strata &lt;- x[1] xx.missing.strata$sample_count&lt;- 0 nn=0 while (sum(xx.missing.strata$sample_count) &lt; group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*5) { while(nn &lt; group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3){ my.missing.strata &lt;- spatSample(x = vect(group_areas[group_areas$code %in% i,]), size = group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*5, method = &quot;regular&quot;) nn &lt;- nn + nrow(data.frame(my.missing.strata)) } xx.missing.strata &lt;- rbind(xx.missing.strata,my.missing.strata) print(sum(xx.missing.strata$sample_count)) } print(i) print(xx.missing.strata) x.missing.strata &lt;- rbind(x.missing.strata,xx.missing.strata) } # Join initial sampling with missing sampling strata data x &lt;- rbind(x, x.missing.strata) # Compute sampling points for missing samples (regular sampling) x.missing.sample &lt;- x[1] for(i in missing.sample){ xx.missing.sample &lt;- x[1] xx.missing.sample$sample_count&lt;- 0 while (sum(xx.missing.sample$sample_count) &lt; (group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3)) { my.missing.sample &lt;- spatSample(x = vect(group_areas[group_areas$code %in% i,]), size = as.numeric(vect(group_areas[group_areas$code %in% i,])[[4]])+ (group_areas[group_areas$code==i,][[&quot;sample_count&quot;]]*3), method = &quot;regular&quot;) xx.missing.sample &lt;- rbind(xx.missing.sample,my.missing.sample) print(sum(xx.missing.sample$sample_count)) } print(i) print(xx.missing.sample) x.missing.sample &lt;- rbind(x.missing.sample,xx.missing.sample) } # Join initial sampling with missing sampling strata data and with missing samples x &lt;- rbind(x, x.missing.sample) # Remove extra artificial replacements x &lt;- x[x$sample_count &gt; 0,] # Convert and export to shapefile z &lt;- x %&gt;% st_as_sf() %&gt;% dplyr::group_by(code) %&gt;% dplyr::mutate(sample_count = as.numeric(sample_count), order = seq_along(code), ID = paste0(code, &quot;.&quot;, order), type = ifelse(sample_count &gt;= order, &quot;Target&quot;, &quot;Replacement&quot;)) %&gt;% vect() writeVector(z, paste0(results.path,&quot;strat_randm_samples.shp&quot;, overwrite=TRUE)) # Check if the number of initial target points equals the final target points n==nrow(z[z$type==&quot;Target&quot;,]) n;nrow(z[z$type==&quot;Target&quot;,]) # Plot results map = leaflet(options = leafletOptions(minZoom = 11.4)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(sf::st_as_sf(z), zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;, &#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Samples&quot;) mv@map ## Random Sampling based on a stratified raster strata &lt;- st_read(paste0(results.path,&quot;strata.shp&quot;),quiet = TRUE) strata$code &lt;- as.integer(strata$code) # Create stratification raster strata &lt;- rast(st_rasterize(strata[&quot;code&quot;],st_as_stars(st_bbox(strata), nx = 250, ny = 250))) names(strata) &lt;- &quot;strata&quot; strata &lt;- crop(strata, nghe, mask=TRUE) # Create stratified random sampling target &lt;- sample_strat( sraster = strata, nSamp = 200 ) target$type &lt;- &quot;target&quot; # Histogram of frequencies for targets calculate_representation( sraster = strata, existing = target, drop=0, plot = TRUE ) # Add index by strata target &lt;- target %&gt;% st_as_sf() %&gt;% dplyr::group_by(strata) %&gt;% dplyr::mutate(sample_count = sum(n), order = seq_along(strata), ID = paste0(strata, &quot;.&quot;, order)) %&gt;% vect() # Histogram of frequencies for replacements calculate_representation( sraster = strata, existing = replacement, drop=0, plot = TRUE ) # Add index by strata replacement &lt;- replacement %&gt;% st_as_sf() %&gt;% dplyr::group_by(strata) %&gt;% dplyr::mutate(sample_count = sum(n), order = seq_along(strata), ID = paste0(strata, &quot;.&quot;, order)) %&gt;% vect() replacement &lt;- sample_strat( sraster = strata, nSamp = 200*3 ) replacement$type &lt;- &quot;replacement&quot; # Plot samples over strata # plot(strata, main=&quot;Strata and random samples&quot;) # plot(nghe[1], col=&quot;transparent&quot;, add=TRUE) # points(target,col=&quot;red&quot;) # points(replacement,col=&quot;dodgerblue&quot;) # legend(&quot;topleft&quot;, legend = c(&quot;target&quot;,&quot;replacement&quot;), pch = 20, xpd=NA, bg=&quot;white&quot;, col=c(&quot;red&quot;,&quot;dodgerblue&quot;)) map = leaflet(options = leafletOptions(minZoom = 8.3)) %&gt;% addTiles() mv &lt;- mapview(soil_lc[&quot;soil_lc&quot;], alpha=0, homebutton=T, layer.name = &quot;Strata&quot;) + mapview(target, zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;tomato&#39;), cex=3, legend = TRUE,layer.name = &quot;Target&quot;) + mapview(replacement, zcol = &#39;type&#39;, color = &quot;white&quot;, col.regions = c(&#39;royalblue&#39;), cex=3, legend = TRUE,layer.name = &quot;Replacement&quot;) mv@map Script 6: Conditional Latin Hypercube Sampling # # Digital Soil Mapping # Soil Sampling Design # conditional Latin Hypercube Sampling # # GSP-Secretariat # Contact: Luis.RodriguezLado@fao.org #________________________________________________________________ # Empty environment and cache rm(list = ls()) gc() # Content of this script ======================================== # Script for creating a sampling design based on conditioned Latin Hypercube Sampling. # Given a suite of covariates this algorithm will assess the optimal location of # samples based on the amount of information in the set of covariates. # # 0 - Set working directory and load packages # 1 - User-defined variables # 2 - Import national data # 3 - Compute clhs # 4 - Including existing legacy data in a cLHS sampling design # 5 - Working with large raster data # 6 - Cost–constrained cLHS sampling # 7 - Replacement areas in cLHS design # 8 - Polygonize replacement areas by similarity # 9 - Constrained cLHS sampling accounting for accessibility and legacy data #________________________________________________________________ start_time &lt;- Sys.time() ## 0 - Set working directory and load packages ================================= #remotes::install_github(&quot;lemuscanovas/synoptReg&quot;) # Set working directory to source file location setwd(dirname(rstudioapi::getActiveDocumentContext()$path)) setwd(&quot;../&quot;) # Move wd down to main folder # List of packages packages &lt;- c(&quot;sp&quot;,&quot;terra&quot;,&quot;raster&quot;,&quot;sf&quot;,&quot;clhs&quot;, &quot;sgsR&quot;,&quot;entropy&quot;, &quot;tripack&quot;, &quot;manipulate&quot;,&quot;dplyr&quot;,&quot;synoptReg&quot;) # Load packages lapply(packages, require, character.only = TRUE) # Remove object to save memory space rm(packages) ## 1 - User-defined variables ================================================== # Path to rasters raster.path &lt;- &quot;data/rasters/&quot; # Path to shapes shp.path &lt;- &quot;data/shapes/&quot; # Path to results results.path &lt;- &quot;data/results/&quot; # Path to additional data other.path &lt;- &quot;data/other/&quot; # Buffer distance for replacement areas (clhs) D &lt;- 1000 # Buffer distance to calculate replacement areas # Define the minimum sample size. By default it uses the value calculated previously minimum_n &lt;- 180 # Aggregation factor for up-scaling raster covariates (optional) agg.factor = 10 ## 2 - Import national data ==================================================== # Read Spatial data covariates as rasters with terra cov.dat &lt;- list.files(raster.path, pattern = &quot;tif$&quot;, recursive = TRUE, full.names = TRUE) cov.dat &lt;- terra::rast(cov.dat) # SpatRaster from terra # Load shape of district nghe &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/Nghe_An.shp&quot;)),quiet=TRUE) # Crop covariates on administrative boundary cov.dat &lt;- crop(cov.dat, nghe, mask=TRUE) # Store elevation and slope separately elevation &lt;- cov.dat$dtm_elevation_250m slope &lt;- cov.dat$dtm_slope_250m # Load roads roads &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/roads.shp&quot;)),quiet=TRUE) roads &lt;- st_intersection(roads, nghe) # Simplify raster information with PCA pca &lt;- raster_pca(cov.dat) # Get SpatRaster layers cov.dat &lt;- pca$PCA # Subset rasters cov.dat &lt;- pca$PCA[[1:first(which(pca$summaryPCA[3,]&gt;0.99))]] # Remove pca stack rm(pca) # Aggregate stack to simplify data rasters for calculations # cov.dat &lt;- aggregate(cov.dat, fact=10, fun=&quot;mean&quot;) # Plot of covariates plot(cov.dat) ## 3 - Compute clhs ============================================================ # Distribute sampling points with clhs pts &lt;- sample_clhs(cov.dat, nSamp = 100, iter = 10000, progress = FALSE, simple = FALSE) # Plot cLHS samples on map plot(cov.dat[[1]], main=&quot;cLHS samples&quot;) points(pts, col=&quot;red&quot;, pch = 1) ## 4 - Including existing legacy data in a cLHS sampling design ================ # Create an artificial random legacy dataset of 50 samples over the study area as an example legacy.data &lt;- sample_srs( raster = cov.dat, nSamp = 50) # Calculate clhs 100 points plus locations of legacy data res &lt;- sample_clhs(cov.dat, nSamp = 150, existing=legacy.data, iter = 10000, progress = FALSE) # Plot points plot(cov.dat[[1]], main=&quot;cLHS samples (blue circles) and legacy samples (red diamonds)&quot;) points(res, col=&quot;navy&quot;, pch = 1) points(res[res$type==&quot;existing&quot;,], col=&quot;red&quot;, pch = 5, cex=2) ## 5 - Working with large raster data ========================================== # Scaling covariates # Aggregation of covariates by a factor of 10. # The original grid resolution is up-scaled using the mean value of the pixels in the grid cov.dat2 &lt;- aggregate(cov.dat, fact=10, fun=&quot;mean&quot;) # Create clhs samples upon the resamples rasters resampled.clhs &lt;- sample_clhs(cov.dat2, nSamp = 150, iter = 10000, progress = FALSE) # Plot the points over the 1st raster plot(cov.dat2[[1]], main=&quot;Regular resampled data&quot;) points(resampled.clhs , col=&quot;red&quot;, pch = 1) rm(cov.dat2) # Sampling to regular points # Create a regular grid of 1000 points on the covariate space regular.sample &lt;- spatSample(cov.dat, size = 1000, xy=TRUE, method=&quot;regular&quot;, na.rm=TRUE) # plot the points over the 1st raster plot(cov.dat[[1]], main=&quot;Regular resampled data&quot;) points(regular.sample, col=&quot;red&quot;, pch = 1) # Create clhs samples upon the regular grid regular.sample.clhs &lt;- clhs(regular.sample, size = 100, progress = FALSE, iter = 10000, simple = FALSE) # Plot points of clhs samples points &lt;- regular.sample.clhs$sampled_data # Get point coordinates of clhs sampling plot(cov.dat[[1]], main=&quot;cLHS samples (red) and covariate resampled points (blue)&quot;) points(regular.sample, col=&quot;navy&quot;, pch = 1) points(points, col=&quot;red&quot;, cex=1) ## 6 - Cost–constrained cLHS sampling # Use &#39;Distance to roads&#39; as cost surface # Calculate distance to roads with te same spatial definition than the covariates # dist2access &lt;- terra::distance(cov.dat[[1]], roads, progress=TRUE) # names(dist2access) &lt;- &quot;dist2access&quot; # Save cost surface to disk # writeRaster(dist2access, paste0(other.path,&quot;nghe_d2roads.tif&quot;), overwrite=TRUE) # Load pre-calculated distance–to–roads surface dist2access &lt;- terra::rast(paste0(other.path,&quot;nghe_d2roads.tif&quot;)) # Aggregate to the same soatial definition # dist2access &lt;- aggregate(dist2access, fact=10, fun=&quot;mean&quot;) plot(dist2access) plot(nghe, col=&quot;transparent&quot;, add=TRUE) # Add cost surface (distance to roads) as layer cov.dat &lt;- c(cov.dat,dist2access) names(cov.dat) # Harmonize NAs cov.dat$dist2access &lt;- cov.dat$dist2access * cov.dat[[1]]/cov.dat[[1]] plot(cov.dat$dist2access) plot(nghe, col=&quot;transparent&quot;,add=TRUE) # Compute sampling points cost.clhs &lt;- sample_clhs(cov.dat, nSamp = minimum_n, iter = 10000, progress = FALSE, cost = &#39;dist2access&#39;, use.cpp = TRUE) # Plot samples plot(cov.dat[[&#39;dist2access&#39;]], main=&quot;cLHS samples with &#39;cost&#39; constraints&quot;) plot(cost.clhs, col=&quot;red&quot;, cex=1,add=T) ## 7 - Replacement areas in cLHS design (requires raster package) # Determine the similarity to points in a buffer of distance &#39;D&#39; # Compute the buffers around points # cov25?? gw &lt;- similarity_buffer(raster::stack(cov.dat) , as(cost.clhs, &quot;Spatial&quot;), buffer = D) # Plot results plot(elevation, legend=TRUE,main=paste(&quot;Similarity probability over elevation&quot;)) ## Overlay points points(cost.clhs, col = &quot;dodgerblue&quot;, pch = 3) ## Overlay probability stack for point 1 colors &lt;- c((RColorBrewer::brewer.pal(9, &quot;YlOrRd&quot;))) terra::plot(gw[[1]], add=TRUE , legend=FALSE, col=colors) ## Overlay 1st cLHS point points(cost.clhs[1,], col = &quot;red&quot;, pch = 12,cex=1) # 8 - Polygonize replacement areas by similarity # Determine a threshold break to delineate replacement areas similarity_threshold &lt;- 0.90 # Reclassify buffer raster data according to the threshold break of probability # 1 = similarity &gt;= similarity_break; NA = similarity &lt; similarity_break # Define a vector with the break intervals and the output values (NA,1) breaks &lt;- c(0, similarity_threshold, NA, similarity_threshold, 1, 1) # Convert to a matrix breaks &lt;- matrix(breaks, ncol=3, byrow=TRUE) # Reclassify the data in the layers from probabilities to (NA,) s = stack(lapply(1:raster::nlayers(gw), function(i){raster::reclassify(gw[[i]], breaks, right=FALSE)})) # Polygonize replacement areas s = lapply(as.list(s), rasterToPolygons, dissolve=TRUE) s &lt;- bind(s,keepnames=TRUE) # Add the identifier of the corresponding target point for(i in 1: length(s)){ s@data$ID[i] &lt;- as.integer(stringr::str_replace(s@polygons[[i]]@ID,&quot;1.&quot;,&quot;&quot;)) } # Clean the data by storing target ID data only s@data &lt;- s@data[&quot;ID&quot;] # Plot results plot(cov.dat[[1]], main=paste(&quot;cLHS samples and replacement areas for threshold = &quot;, similarity_threshold)) plot(s,add=TRUE, col=NA, border=&quot;gray40&quot;) points(cost.clhs, col=&quot;red&quot;, pch = 3) # Export replacement areas to shapefile s &lt;- st_as_sf(s) st_write(s, file.path(paste0(results.path,&#39;replacement_areas_&#39;, D, &#39;.shp&#39;)), delete_dsn = TRUE) # Write cLHS sampling points to shapefile cost.clhs$ID &lt;- row(cost.clhs)[,1] # Add identifier out.pts &lt;- st_as_sf(cost.clhs) st_write(out.pts, paste0(results.path,&#39;target_clhs.shp&#39;), delete_dsn = TRUE) ## 9 - Constrained cLHS sampling taking into account accessibility and legacy data # Load legacy data legacy &lt;- sf::st_read(file.path(paste0(shp.path,&quot;/legacy_soils.shp&quot;)),quiet=TRUE) # Add slope to the stack cov.dat$slope &lt;- slope # Calculate clhs points with legacy, cost and buffer to roads buff_inner=20; buff_outer=3000 # Convert roads to sf object and cast to multilinestring roads &lt;- st_as_sf(roads) %&gt;% st_cast(&quot;MULTILINESTRING&quot;) # Calculate clhs samples using slope as cost surface, distance to roads as # access limitations, and including existing legacy data aa &lt;- sgsR::sample_clhs(mraster = cov.dat, nSamp = minimum_n, existing = legacy, iter = 10000, details = TRUE, cost=&quot;slope&quot;, access=roads, buff_inner=buff_inner, buff_outer=buff_outer) ## Plot distances, roads, clhs points and legacy data plot(cov.dat$dist2access, main=&quot;New (red) and existing (blue) samples&quot;) plot(roads,add=TRUE, col=&quot;gray60&quot;) plot(aa$samples[aa$samples$type==&quot;new&quot;,], col= &quot;tomato&quot;,add=TRUE) plot(aa$samples[aa$samples$type==&quot;existing&quot;,], col= &quot;dodgerblue2&quot;,add=TRUE) # Write samples as shapefile aa$samples[c(&quot;type&quot;,&quot;dist2access&quot;)] %&gt;% st_write(paste0(results.path,&#39;const_clhs.shp&#39;), delete_dsn = TRUE) ## 10 - Calculate COOBS (sgsR) ================================================= #&#39; COOBS (count of observations) is an algorithm to map relatively #&#39; adequate and under-sampled areas on a sampling pattern (generally clhs) calculate_coobs( mraster = cov.dat, existing = cost.clhs, plot = TRUE, cores = 4 ) ## 11 - Plot sample density over covariates ==================================== # Distribution of elevation values on the sampling design vs. original elevation data s0 &lt;- rbind(data.frame(method = &quot;Elevation&quot;, data.frame(elevation))) s1 &lt;- extract(elevation,pts) s1$method &lt;-&quot;cLHS&quot; # Plot the covariate elevation values as histogram bars and the sampling distribution as curve library(ggplot2) dens_elev &lt;- ggplot() + geom_histogram(data=s0, aes(x=dtm_elevation_250m,fill=&quot;red&quot;,y=after_stat(density)),binwidth=25, alpha=.5, position=&quot;identity&quot;) + geom_density(data = s1, aes(x = dtm_elevation_250m, col = method)) dens_elev "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
