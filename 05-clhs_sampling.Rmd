
# Conditioned Latin Hypercube Sampling 

Conditioned Latin Hypercube Sampling (cLHS) is an advanced statistical method used for sampling multidimensional data developed within the context of digital Soil Mapping. It's an extension of the basic Latin Hypercube Sampling (LHS) technique, a statistical method for generating a distribution of samples of a random variable. The main advantage of LHS over simple random sampling is its ability to ensure that the entire range of the auxiliary variables are explored. It divides the range of each variable into intervals of equal probability and samples each interval.

The term "conditioned" refers to the way the sampling is adapted or conditioned based on specific requirements or constraints. It often involves conditioning the sampling process on one or more additional variables or criteria. This helps in generating samples that are not just representative in terms of the range of values, but also in terms of their relationships or distributions. cLHS is particularly useful for sampling from multivariate data, where there are multiple interrelated variables as it occurs in soil surveys. The main advantage of cLHS is its efficiency in sampling and its ability to better capture the structure and relationships within the data, compared to simpler sampling methods, and ensures that the samples are representative not just of the range of each variable, but also of their interrelations. Detailed information on cLHS can be found in [@minasny2006]

## cLHS design

As for stratified sampling, the creation target points from a conditioned Latin Hypercube Sampling design involves the identification of the relevant features describing the environmental diversity in the area. In this case, the environmental parameters are incorporated in the form of raster covariates. The determination of the optimal number of samples in the design is also required. This step can be calculated following the information already provided in this manual.

With the optimal sampling size of `r optimal_n` calculated before, we can conduct conditioned Latin Hypercube Sampling design for the area in the example using the R package `'cLHS'` available at CRAN. 

We use the rasters of `r paste(shQuote(names(cov.dat)), collapse = ", ")` as covariates in the exercise, which we convert to a raster.


```{r load_data_05, eval=TRUE, include=TRUE, warning=FALSE,echo=FALSE}

  ## Load raster covariate data----
  # Read Spatial data covariates as rasters with terra
  library(raster)
  rasters <- "data/rasters"
  cov.dat <-  list.files(rasters, pattern = "tif$",  recursive = TRUE, full.names = TRUE)
  cov.dat <- terra::rast(cov.dat)
  cov.dat <- raster::stack(cov.dat)
  
```


The target points are obtained using the `'cLHS'`function together with the stack of raster covariates and the number of samples. The function uses a number of iterations for the Metropolis-Hastings annealing process, with a default of 10000.

```{r cLHS_design_05, eval=TRUE, include=TRUE}

  ## Create a cLHS sampling point set----
    pts <- clhs(cov.dat, size = optimal_n, iter = 10000, progress = FALSE, simple = FALSE)

```

The distribution of points is shown in Figure \@ref(fig:fig-15).

```{r fig-15, fig.cap="Distribution of cLHS sampling points in the study area", eval=TRUE, include=TRUE}

  ## Create a cLHS sampling point set----
    plot(cov.dat[[1]], main="cLHS samples")
    points(pts$sampled_data, col="red", pch = 16)

```


## Replacement areas in cLHS design

The `'cLHS'` package incorporates methods for the delineation of replacement locations that could be utilized in the case any sampling point is unreachable. In this case, the function determines the probability of similarity to each point in an area determined by a buffer distance around the points. 

```{r cLHS_buffer_05, eval=TRUE, include=TRUE}

  ## Determine the similarity to points in a buffer of distance D
  # Note that distance units are expressed in map units, in this case meters
  # If your data is in geographic coordinates, D must be included in geographic units as well
    D <- 50

  ## Compute the buffers around points  
    gw <- similarity_buffer(cov.dat, pts$sampled_data, buffer = D)
    ## Plot the first buffers in the stack
      #plot(gw)
  
  ## Overlay point location to buffer plots
    # fun <- function() {
    #   plot(pts$sampled_data, add = TRUE, col = "red", pch = 3)
    # }
    # plot(gw, addfun = fun)

```

The similarity probabilities for the first cLHS point is presented on Figure \@ref(fig:fig-16).over the elevation layer. 

```{r fig-16, fig.cap="Probability of imilarity in the buffer for the first cLHS point (in black) over elevation. The blue crosses represent the location of the remaining cLHS points from the analysis.", eval=TRUE, include=TRUE}

  ## Plot elevation
    plot(cov.dat[[3]], legend=TRUE,main=paste("Similarity Probability over elevation"))

  ## Overlay points
    points(pts$sampled_data[1], col = "dodgerblue", pch = 3)
  ## Overlay probability stack for point 1
    colors <- c((RColorBrewer::brewer.pal(9, "YlOrRd")))
    terra::plot(gw[[1]], add=TRUE ,  legend=FALSE, col=colors)
  ## Overlay 1st cLHS point
    points(pts$sampled_data[1,1], col = "black", pch = 3,cex=1)

```

The probabilities can then be reclassified using a threshold value to delineate the areas with higher similarity to each central target point. 

```{r cLHS_reclass_05, eval=TRUE, include=TRUE}
  ## Determine the threshold break to determine if the surrounding area can be a replacement or not
    similarity_threshold <- 0.90
  ## Reclassify buffer raster data according to the threshold break of probability
  # 1 = similarity >= similarity_break; NA =  similarity <  similarity_break
    # Define a vector with the break intervals and the output values (NA,1) 
    breaks <- c(0, similarity_threshold, NA, similarity_threshold, 1, 1)
    # Convert to a matrix
    breaks <- matrix(breaks, ncol=3, byrow=TRUE)
    # Reclassify the data in the layers from probabilities to (NA,)
    s = stack(lapply(1:raster::nlayers(gw), function(i){reclassify(gw[[i]], breaks, right=FALSE)}))
    #s
  # Plot points over reclassified rasters 
    #plot(s, addfun = fun)
        
  ## Remove temporary objects
    rm(gw,breaks)

```


The reclassified raster stack is then converted to an object of `'SpatialPolygonsDataFrame'` class.

```{r cLHS_polygonize_05, eval=TRUE, include=TRUE}

   ## Polygonize replacement areas 
    s = lapply(as.list(s), rasterToPolygons, dissolve=TRUE)
    s <- bind(s,keepnames=TRUE)
    
    # Add the identifier of the corresponding target point
    for(i in 1: length(s)){
      s@data$ID[i] <- as.numeric(gsub("1.","",s@polygons[[i]]@ID))
    }
    # Clean the data by storing target ID data only
    s@data <- s@data["ID"]

  
```


The results are shown in Figure \@ref(fig:fig-17).

```{r fig-17, fig.cap="Distribution of cLHS sampling points in the study area", eval=TRUE, include=TRUE}

    plot(cov.dat[[1]], main=paste("cLHS samples and replacement areas for threshold = ", similarity_threshold))
    plot(s,add=TRUE, col=NA, border="gray40")
    points(pts$sampled_data, col="red", pch = 16)

```


The layer of replacement areas can finally be exported to a shapefile to be used in any other software.

```{r cLHS_export_05, eval=FALSE, include=TRUE}

   ## Write replacement areas to polygon shape 
    writeSpatialShape(s, "replacement_areas")
    
   ## Write  cLHS sampling points to shapefile
    writeSpatialShape(pts$sampled_data, "points")

```
